#!/usr/bin/env python3
"""
Meter Reading Extraction Web Application
A complete pipeline for extracting readings from industrial meter displays

Pipeline:
1. Upload image -> YOLO detection -> Crop meter region
2. Crop -> DeepLabV3+ segmentation -> Generate masks
3. Masks -> Reading extraction -> Final result

Author: AI Assistant
Date: 2024
"""

import gradio as gr
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from ultralytics import YOLO
import matplotlib.pyplot as plt
from pathlib import Path
import tempfile
import os
import sys
from typing import Tuple, Optional, List, Dict, Any
import json
from PIL import Image
import io
import base64

# Add scripts directory to path
sys.path.append(str(Path(__file__).parent / "scripts"))
from scripts.extract_meter_reading import MeterReader

# Import ONNX runtime for segmentation
import onnxruntime as ort


class MeterDetector:
    """YOLO-based meter detection"""
    
    def __init__(self, model_path: str):
        """Initialize detector with trained model"""
        self.model = YOLO(model_path)
        
    def detect_meters(self, image: np.ndarray, conf_threshold: float = 0.5) -> List[Dict]:
        """
        Detect meters in image
        
        Args:
            image: Input image (BGR format)
            conf_threshold: Confidence threshold
            
        Returns:
            List of detection results with bounding boxes
        """
        results = self.model(image, conf=conf_threshold)
        
        detections = []
        for r in results:
            boxes = r.boxes
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    
                    detections.append({
                        'bbox': [int(x1), int(y1), int(x2), int(y2)],
                        'confidence': float(conf),
                        'class': 'meter'
                    })
        
        return detections
    
    def crop_meter(self, image: np.ndarray, bbox: List[int], padding: int = 20) -> np.ndarray:
        """
        Crop meter region from image
        
        Args:
            image: Input image
            bbox: Bounding box [x1, y1, x2, y2]
            padding: Padding around bounding box
            
        Returns:
            Cropped meter image
        """
        x1, y1, x2, y2 = bbox
        h, w = image.shape[:2]
        
        # Add padding
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(w, x2 + padding)
        y2 = min(h, y2 + padding)
        
        return image[y1:y2, x1:x2]


class MeterSegmentor:
    """ONNX-based meter segmentation"""
    
    def __init__(self, model_path: str, device: str = 'cpu', post_process_config: Dict = None):
        """Initialize segmentor with ONNX model"""
        self.device = device
        self.session = self._load_onnx_model(model_path)
        
        # ÂêéÂ§ÑÁêÜÈÖçÁΩÆ
        self.post_process_config = post_process_config or {
            'remove_noise': True,           # ÊòØÂê¶ÂéªÈô§Âô™Â£∞
            'keep_largest_component': False, # ÊòØÂê¶Âè™‰øùÁïôÊúÄÂ§ßËøûÈÄöÂüü
            'pointer_erosion': 1,           # ÊåáÈíàËÖêËöÄËø≠‰ª£Ê¨°Êï∞
            'scale_erosion': 3,             # ÂàªÂ∫¶ËÖêËöÄËø≠‰ª£Ê¨°Êï∞
            'fill_holes': False,             # ÊòØÂê¶Â°´ÂÖÖÂ∞èÊ¥û
            'connect_scale_lines': False     # ÊòØÂê¶ËøûÊé•Êñ≠Ë£ÇÁöÑÂàªÂ∫¶Á∫ø
        }
        
    def _load_onnx_model(self, model_path: str):
        """Load ONNX segmentation model"""
        # Check if ONNX file exists
        onnx_path = model_path.replace('.pth', '.onnx')
        if not os.path.exists(onnx_path):
            # Try the exported directory
            onnx_path = "models/segmentation/segmentation_model.onnx"
        
        if os.path.exists(onnx_path):
            try:
                # Configure ONNX Runtime providers
                providers = []
                if self.device == 'cuda' and 'CUDAExecutionProvider' in ort.get_available_providers():
                    providers.append('CUDAExecutionProvider')
                elif self.device == 'mps':
                    # ONNX Runtime doesn't support MPS directly, use CPU
                    providers.append('CPUExecutionProvider')
                else:
                    providers.append('CPUExecutionProvider')
                
                # Create inference session
                session = ort.InferenceSession(onnx_path, providers=providers)
                print(f"‚úÖ Loaded ONNX model from: {onnx_path}")
                print(f"üìä Input shape: {session.get_inputs()[0].shape}")
                print(f"üìä Output shape: {session.get_outputs()[0].shape}")
                print(f"üîß Providers: {session.get_providers()}")
                
                return session
                
            except Exception as e:
                print(f"‚ùå Error loading ONNX model: {e}")
                return None
        else:
            print(f"‚ö†Ô∏è  ONNX model not found at: {onnx_path}")
            return None
    
    def preprocess_image(self, image: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
        """Preprocess image for ONNX inference"""
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Resize image
        image_resized = cv2.resize(image_rgb, target_size)
        
        # Normalize to [0, 1] and then apply ImageNet normalization
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # ImageNet normalization
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_normalized = (image_normalized - mean) / std
        
        # Convert to NCHW format
        image_tensor = np.transpose(image_normalized, (2, 0, 1))
        image_batch = np.expand_dims(image_tensor, axis=0)
        
        return image_batch.astype(np.float32)
    
    def post_process_mask(self, mask: np.ndarray) -> np.ndarray:
        """
        ÂØπÂàÜÂâ≤ÁªìÊûúËøõË°åÂêéÂ§ÑÁêÜÔºåÂéªÈô§Á¶ªÁæ§ÁÇπÂíå‰ºòÂåñËæπÁïå
        
        Args:
            mask: ÂéüÂßãÂàÜÂâ≤Êé©Á†Å
            
        Returns:
            Â§ÑÁêÜÂêéÁöÑÂàÜÂâ≤Êé©Á†Å
        """
        if not any(self.post_process_config.values()):
            return mask  # Â¶ÇÊûúÊâÄÊúâÂêéÂ§ÑÁêÜÈÉΩÂÖ≥Èó≠ÔºåÁõ¥Êé•ËøîÂõûÂéüÊé©Á†Å
            
        processed_mask = mask.copy()
        config = self.post_process_config
        
        # ‰∏∫ÊØè‰∏™Á±ªÂà´ÂàÜÂà´Â§ÑÁêÜ
        for class_id in [1, 2]:  # ÊåáÈíàÂíåÂàªÂ∫¶
            if class_id not in mask:
                continue
                
            # ÊèêÂèñÂΩìÂâçÁ±ªÂà´ÁöÑÊé©Á†Å
            class_mask = (mask == class_id).astype(np.uint8)
            
            if np.sum(class_mask) == 0:
                continue
            
            # 1. ÂéªÈô§Â∞èÁöÑÁ¶ªÁæ§ÁÇπ - ÂºÄËøêÁÆóÔºàÂÖàËÖêËöÄÂêéËÜ®ËÉÄÔºâ
            if config['remove_noise']:
                kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_OPEN, kernel_small)
            
            # 2. ËøûÈÄöÂüüÂàÜÊûêÔºå‰øùÁïôÊúÄÂ§ßÁöÑËøûÈÄöÂå∫Âüü
            if config['keep_largest_component']:
                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(class_mask, connectivity=8)
                
                if num_labels > 1:  # ÊúâËøûÈÄöÂüüÔºàÈô§‰∫ÜËÉåÊôØÔºâ
                    # ÊâæÂà∞ÊúÄÂ§ßÁöÑËøûÈÄöÂüüÔºàÊéíÈô§ËÉåÊôØÊ†áÁ≠æ0Ôºâ
                    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
                    class_mask = (labels == largest_label).astype(np.uint8)
            
            # 3. Ê†πÊçÆÁ±ªÂà´ËøõË°åÁâπÂÆöÂ§ÑÁêÜ
            if class_id == 1:  # ÊåáÈíà
                # ÊåáÈíàÈúÄË¶ÅÁªÜÂåñÔºå‰ΩøÁî®ËæÉÂ∞èÁöÑËÖêËöÄÊ†∏
                if config['pointer_erosion'] > 0:
                    kernel_pointer = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                    class_mask = cv2.erode(class_mask, kernel_pointer, iterations=config['pointer_erosion'])
                
            elif class_id == 2:  # ÂàªÂ∫¶
                # ÂàªÂ∫¶ÈúÄË¶ÅÊõ¥Â§öËÖêËöÄÊù•Êî∂Áº©ËæπÁïåÔºåÈò≤Ê≠¢Â§ñÁßª
                if config['scale_erosion'] > 0:
                    kernel_scale = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                    class_mask = cv2.erode(class_mask, kernel_scale, iterations=config['scale_erosion'])
                
                # ÂØπÂàªÂ∫¶ËøõË°åÈ¢ùÂ§ñÁöÑÂΩ¢ÊÄÅÂ≠¶Èó≠ËøêÁÆóÔºåËøûÊé•Êñ≠Ë£ÇÁöÑÂàªÂ∫¶Á∫ø
                if config['connect_scale_lines']:
                    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                    class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel_close)
            
            # 4. Â°´ÂÖÖÂ∞èÊ¥û
            if config['fill_holes']:
                kernel_fill = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel_fill)
            
            # Â∞ÜÂ§ÑÁêÜÂêéÁöÑÁ±ªÂà´Êé©Á†ÅÊîæÂõûÂéüÊé©Á†Å
            processed_mask[mask == class_id] = 0  # ÂÖàÊ∏ÖÈô§ÂéüÊù•ÁöÑ
            processed_mask[class_mask == 1] = class_id  # ÂÜçÊîæÂÖ•Â§ÑÁêÜÂêéÁöÑ
        
        return processed_mask
    
    def segment_meter(self, image: np.ndarray) -> np.ndarray:
        """
        Segment meter image into classes using ONNX
        
        Args:
            image: Input meter image (BGR format)
            
        Returns:
            Segmentation mask with class labels
        """
        if self.session is None:
            print("‚ö†Ô∏è  No ONNX model loaded, returning dummy mask")
            # Return a dummy mask with some basic segmentation
            h, w = image.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            # Create a simple circular mask as placeholder
            center = (w//2, h//2)
            radius = min(w, h) // 4
            cv2.circle(mask, center, radius, 1, -1)  # pointer region
            cv2.circle(mask, center, radius + 20, 2, 10)  # scale region
            return mask
        
        original_size = image.shape[:2]
        
        # Preprocess
        input_data = self.preprocess_image(image)
        
        # Get input name
        input_name = self.session.get_inputs()[0].name
        
        # Run inference
        try:
            outputs = self.session.run(None, {input_name: input_data})
            output = outputs[0]  # First output
            
            # Get predictions (argmax along channel dimension)
            predictions = np.argmax(output, axis=1).squeeze(0).astype(np.uint8)
            
            # Resize back to original size
            mask = cv2.resize(predictions, 
                             (original_size[1], original_size[0]), 
                             interpolation=cv2.INTER_NEAREST)
            
            # ÂêéÂ§ÑÁêÜÔºöÂéªÈô§Á¶ªÁæ§ÁÇπÂíå‰ºòÂåñËæπÁïå
            mask = self.post_process_mask(mask)
            
            return mask
            
        except Exception as e:
            print(f"‚ùå ONNX inference error: {e}")
            # Return dummy mask on error
            h, w = image.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            return mask


class MeterReadingApp:
    """Complete meter reading application"""
    
    def __init__(self):
        """Initialize the application"""
        # Model paths
        self.detection_model_path = "models/detection/detection_model.pt"
        self.segmentation_model_path = "models/segmentation/segmentation_model.onnx"
        
        # Fallback to base models if trained models not available
        if not os.path.exists(self.detection_model_path):
            self.detection_model_path = "yolov10n.pt"
            print("Using base YOLOv10 model (not trained on meters)")
        
        # Initialize components
        self.detector = MeterDetector(self.detection_model_path)
        
        # Determine device
        self.device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        
        self.segmentor = MeterSegmentor(self.segmentation_model_path, self.device)
        self.reader = MeterReader(scale_range=(0.0, 1.6), debug=False)
        
    def process_image(self, image: np.ndarray, conf_threshold: float = 0.5, 
                     scale_min: float = 0.0, scale_max: float = 1.6) -> Dict[str, Any]:
        """
        Complete processing pipeline
        
        Args:
            image: Input image
            conf_threshold: Detection confidence threshold
            scale_min: Minimum scale value
            scale_max: Maximum scale value
            
        Returns:
            Dictionary with all results and visualizations
        """
        results = {
            'success': False,
            'error': None,
            'detections': [],
            'readings': [],
            'visualizations': {}
        }
        
        try:
            # Step 1: Detection
            detections = self.detector.detect_meters(image, conf_threshold)
            results['detections'] = detections
            
            if not detections:
                results['error'] = "No meters detected in the image"
                return results
            
            # Process each detected meter
            for i, detection in enumerate(detections):
                try:
                    # Step 2: Crop meter region
                    cropped_meter = self.detector.crop_meter(image, detection['bbox'])
                    
                    # Step 3: Segmentation
                    segmentation_mask = self.segmentor.segment_meter(cropped_meter)
                    
                    # Step 4: Reading extraction
                    self.reader.scale_beginning = scale_min
                    self.reader.scale_end = scale_max
                    reading = self.reader.process_single_meter(cropped_meter, segmentation_mask)
                    
                    if reading is not None:
                        results['readings'].append({
                            'meter_id': i,
                            'reading': reading,
                            'confidence': detection['confidence'],
                            'bbox': detection['bbox']
                        })
                    
                    # Generate visualizations
                    vis_detection = self._visualize_detection(image, [detection])
                    vis_crop = cropped_meter
                    vis_segmentation = self._visualize_segmentation(cropped_meter, segmentation_mask)
                    vis_result = self._visualize_reading_result(cropped_meter, segmentation_mask, reading)
                    
                    results['visualizations'][f'meter_{i}'] = {
                        'detection': vis_detection,
                        'crop': vis_crop,
                        'segmentation': vis_segmentation,
                        'result': vis_result
                    }
                    
                except Exception as e:
                    print(f"Error processing meter {i}: {e}")
                    continue
            
            results['success'] = len(results['readings']) > 0
            if not results['success']:
                results['error'] = "Failed to extract readings from detected meters"
                
        except Exception as e:
            results['error'] = f"Processing error: {str(e)}"
            
        return results
    
    def _visualize_detection(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """Visualize detection results"""
        vis_img = image.copy()
        
        for detection in detections:
            bbox = detection['bbox']
            conf = detection['confidence']
            
            # Draw bounding box
            cv2.rectangle(vis_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
            
            # Draw label
            label = f"Meter: {conf:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(vis_img, (bbox[0], bbox[1] - label_size[1] - 10), 
                         (bbox[0] + label_size[0], bbox[1]), (0, 255, 0), -1)
            cv2.putText(vis_img, label, (bbox[0], bbox[1] - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        return vis_img
    
    def _visualize_segmentation(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Visualize segmentation results"""
        # Create colored mask
        colored_mask = np.zeros_like(image)
        colored_mask[mask == 1] = [0, 0, 255]    # Pointer - Red
        colored_mask[mask == 2] = [0, 255, 0]    # Scale - Green
        
        # Blend with original image
        alpha = 0.6
        vis_img = cv2.addWeighted(image, 1-alpha, colored_mask, alpha, 0)
        
        # Add statistics text
        pointer_pixels = np.sum(mask == 1)
        scale_pixels = np.sum(mask == 2)
        total_pixels = mask.shape[0] * mask.shape[1]
        
        # Add text overlay with statistics
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.4
        thickness = 1
        
        text_lines = [
            f"Pointer: {pointer_pixels} px ({pointer_pixels/total_pixels*100:.1f}%)",
            f"Scale: {scale_pixels} px ({scale_pixels/total_pixels*100:.1f}%)",
            f"Post-processed: Cleaned noise & boundaries"
        ]
        
        y_offset = 15
        for i, text in enumerate(text_lines):
            y_pos = y_offset + i * 15
            # Add background rectangle for better readability
            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
            cv2.rectangle(vis_img, (5, y_pos - 12), (text_size[0] + 10, y_pos + 3), (0, 0, 0), -1)
            cv2.putText(vis_img, text, (8, y_pos), font, font_scale, (255, 255, 255), thickness)
        
        return vis_img
    
    def _visualize_reading_result(self, image: np.ndarray, mask: np.ndarray, reading: Optional[float]) -> np.ndarray:
        """Visualize final reading result"""
        if reading is None:
            return image
        
        # Use the reader's visualization method
        try:
            # Extract components for visualization
            pointer_mask = self.reader.threshold_by_category(mask, 1)
            scale_mask = self.reader.threshold_by_category(mask, 2)
            
            # Find components
            scale_locations = self.reader.get_scale_locations(scale_mask)
            center = self.reader.get_center_location(image)
            pointer_locations = self.reader.get_pointer_locations(pointer_mask, center) if center else None
            
            if all([scale_locations, center, pointer_locations]):
                vis_img = self.reader.visualize_result(image, scale_locations, pointer_locations, center, reading)
                return vis_img
        except:
            pass
        
        # Fallback: simple text overlay
        vis_img = image.copy()
        text = f"Reading: {reading:.3f}"
        cv2.putText(vis_img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        return vis_img


def create_gradio_interface():
    """Create Gradio interface"""
    
    # Initialize app
    app = MeterReadingApp()
    
    def process_uploaded_image(image, conf_threshold, scale_min, scale_max):
        """Process uploaded image and return results"""
        if image is None:
            return None, None, None, None, "Please upload an image"
        
        # Convert PIL to OpenCV format
        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # Process image
        results = app.process_image(image_cv, conf_threshold, scale_min, scale_max)
        
        if not results['success']:
            error_msg = results.get('error', 'Unknown error occurred')
            return None, None, None, None, error_msg
        
        # Prepare outputs
        summary_text = f"Found {len(results['readings'])} meter(s)\n\n"
        for reading in results['readings']:
            summary_text += f"Meter {reading['meter_id']}: {reading['reading']:.3f} (conf: {reading['confidence']:.2f})\n"
        
        # Get visualizations for first meter
        if results['visualizations']:
            first_meter = list(results['visualizations'].keys())[0]
            vis = results['visualizations'][first_meter]
            
            # Convert BGR to RGB for display
            detection_img = cv2.cvtColor(vis['detection'], cv2.COLOR_BGR2RGB)
            crop_img = cv2.cvtColor(vis['crop'], cv2.COLOR_BGR2RGB)
            segmentation_img = cv2.cvtColor(vis['segmentation'], cv2.COLOR_BGR2RGB)
            result_img = cv2.cvtColor(vis['result'], cv2.COLOR_BGR2RGB)
            
            return detection_img, crop_img, segmentation_img, result_img, summary_text
        
        return None, None, None, None, summary_text
    
    # Create interface
    with gr.Blocks(title="Meter Reading Extraction", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # üîß Industrial Meter Reading Extraction
        
        Upload an image containing industrial meters to automatically extract readings using AI.
        
        **Pipeline:** Detection ‚Üí Cropping ‚Üí Segmentation ‚Üí Reading Extraction
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # Input section
                gr.Markdown("## üì§ Input")
                image_input = gr.Image(type="pil", label="Upload Meter Image")
                
                with gr.Row():
                    conf_threshold = gr.Slider(0.1, 0.9, value=0.5, step=0.1, 
                                             label="Detection Confidence")
                    
                with gr.Row():
                    scale_min = gr.Number(value=0.0, label="Scale Min Value")
                    scale_max = gr.Number(value=1.6, label="Scale Max Value")
                
                process_btn = gr.Button("üöÄ Extract Readings", variant="primary", size="lg")
                
                # Results summary
                gr.Markdown("## üìä Results")
                results_text = gr.Textbox(label="Summary", lines=5, interactive=False)
            
            with gr.Column(scale=2):
                # Visualization section
                gr.Markdown("## üëÅÔ∏è Process Visualization")
                
                with gr.Row():
                    detection_output = gr.Image(label="1. Detection Results")
                    crop_output = gr.Image(label="2. Cropped Meter")
                
                with gr.Row():
                    segmentation_output = gr.Image(label="3. Segmentation Masks")
                    result_output = gr.Image(label="4. Final Reading")
        
        # Event handlers
        process_btn.click(
            fn=process_uploaded_image,
            inputs=[image_input, conf_threshold, scale_min, scale_max],
            outputs=[detection_output, crop_output, segmentation_output, result_output, results_text]
        )
        
        # Examples
        gr.Markdown("## üìã Usage Instructions")
        gr.Markdown("""
        1. **Upload Image**: Choose an image containing industrial meters
        2. **Adjust Settings**: 
           - Detection Confidence: Higher values = more strict detection
           - Scale Range: Set the min/max values of your meter scale
        3. **Process**: Click "Extract Readings" to run the complete pipeline
        4. **View Results**: Check the visualization and summary
        
        **Supported Formats**: JPG, PNG, BMP
        **Best Results**: Clear, well-lit images with visible meter faces
        """)
    
    return interface


def main():
    """Main function to launch the application"""
    print("Initializing Meter Reading Extraction App...")
    
    # Create and launch interface
    interface = create_gradio_interface()
    
    print("Launching Gradio interface...")
    interface.launch(
        server_name="0.0.0.0",  # Allow external access
        server_port=7860,       # Default Gradio port
        share=False,            # Set to True for public sharing
        debug=True
    )


if __name__ == "__main__":
    main() 
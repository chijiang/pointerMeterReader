#!/usr/bin/env python3
"""
Meter Reading Extraction Web Application
A complete pipeline for extracting readings from industrial meter displays

Pipeline:
1. Upload image -> YOLO detection -> Crop meter region
2. Crop -> DeepLabV3+ segmentation -> Generate masks
3. Masks -> Reading extraction -> Final result

Author: AI Assistant
Date: 2024
"""

import gradio as gr
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from pathlib import Path
import os
import sys
from typing import Tuple, Optional, List, Dict, Any

# Add scripts directory to path
sys.path.append(str(Path(__file__).parent / "scripts"))
from scripts.extract_meter_reading import MeterReader

# Import ONNX runtime for segmentation
import onnxruntime as ort


class MeterDetector:
    """YOLO-based meter detection"""
    
    def __init__(self, model_path: str):
        """Initialize detector with trained model"""
        self.model = YOLO(model_path)
        
    def detect_meters(self, image: np.ndarray, conf_threshold: float = 0.5) -> List[Dict]:
        """
        Detect meters in image
        
        Args:
            image: Input image (BGR format)
            conf_threshold: Confidence threshold
            
        Returns:
            List of detection results with bounding boxes
        """
        results = self.model(image, conf=conf_threshold)
        
        detections = []
        for r in results:
            boxes = r.boxes
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    
                    detections.append({
                        'bbox': [int(x1), int(y1), int(x2), int(y2)],
                        'confidence': float(conf),
                        'class': 'meter'
                    })
        
        return detections
    
    def crop_meter(self, image: np.ndarray, bbox: List[int], padding: int = 20) -> np.ndarray:
        """
        Crop meter region from image
        
        Args:
            image: Input image
            bbox: Bounding box [x1, y1, x2, y2]
            padding: Padding around bounding box
            
        Returns:
            Cropped meter image
        """
        x1, y1, x2, y2 = bbox
        h, w = image.shape[:2]
        
        # Add padding
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(w, x2 + padding)
        y2 = min(h, y2 + padding)
        
        return image[y1:y2, x1:x2]


class MeterSegmentor:
    """ONNX-based meter segmentation"""
    
    def __init__(self, model_path: str, device: str = 'cpu', post_process_config: Dict = None):
        """Initialize segmentor with ONNX model"""
        self.device = device
        self.session = self._load_onnx_model(model_path)
        
        # åå¤„ç†é…ç½®
        self.post_process_config = post_process_config or {
            'remove_noise': True,           # æ˜¯å¦å»é™¤å™ªå£°
            'keep_largest_component': False, # æ˜¯å¦åªä¿ç•™æœ€å¤§è¿é€šåŸŸ
            'pointer_erosion': 1,           # æŒ‡é’ˆè…èš€è¿­ä»£æ¬¡æ•°
            'scale_erosion': 3,             # åˆ»åº¦è…èš€è¿­ä»£æ¬¡æ•°
            'fill_holes': False,             # æ˜¯å¦å¡«å……å°æ´
            'connect_scale_lines': False     # æ˜¯å¦è¿æ¥æ–­è£‚çš„åˆ»åº¦çº¿
        }
        
    def _load_onnx_model(self, model_path: str):
        """Load ONNX segmentation model"""
        # Check if ONNX file exists
        onnx_path = model_path.replace('.pth', '.onnx')
        if not os.path.exists(onnx_path):
            # Try the exported directory
            onnx_path = "models/segmentation/segmentation_model.onnx"
        
        if os.path.exists(onnx_path):
            try:
                # Configure ONNX Runtime providers
                providers = []
                if self.device == 'cuda' and 'CUDAExecutionProvider' in ort.get_available_providers():
                    providers.append('CUDAExecutionProvider')
                elif self.device == 'mps':
                    # ONNX Runtime doesn't support MPS directly, use CPU
                    providers.append('CPUExecutionProvider')
                else:
                    providers.append('CPUExecutionProvider')
                
                # Create inference session
                session = ort.InferenceSession(onnx_path, providers=providers)
                print(f"âœ… Loaded ONNX model from: {onnx_path}")
                print(f"ğŸ“Š Input shape: {session.get_inputs()[0].shape}")
                print(f"ğŸ“Š Output shape: {session.get_outputs()[0].shape}")
                print(f"ğŸ”§ Providers: {session.get_providers()}")
                
                return session
                
            except Exception as e:
                print(f"âŒ Error loading ONNX model: {e}")
                return None
        else:
            print(f"âš ï¸  ONNX model not found at: {onnx_path}")
            return None
    
    def preprocess_image(self, image: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
        """Preprocess image for ONNX inference"""
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Resize image
        image_resized = cv2.resize(image_rgb, target_size)
        
        # Normalize to [0, 1] and then apply ImageNet normalization
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # ImageNet normalization
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_normalized = (image_normalized - mean) / std
        
        # Convert to NCHW format
        image_tensor = np.transpose(image_normalized, (2, 0, 1))
        image_batch = np.expand_dims(image_tensor, axis=0)
        
        return image_batch.astype(np.float32)
    
    def post_process_mask(self, mask: np.ndarray) -> np.ndarray:
        """
        å¯¹åˆ†å‰²ç»“æœè¿›è¡Œåå¤„ç†ï¼Œå»é™¤ç¦»ç¾¤ç‚¹å’Œä¼˜åŒ–è¾¹ç•Œ
        
        Args:
            mask: åŸå§‹åˆ†å‰²æ©ç 
            
        Returns:
            å¤„ç†åçš„åˆ†å‰²æ©ç 
        """
        if not any(self.post_process_config.values()):
            return mask  # å¦‚æœæ‰€æœ‰åå¤„ç†éƒ½å…³é—­ï¼Œç›´æ¥è¿”å›åŸæ©ç 
            
        processed_mask = mask.copy()
        config = self.post_process_config
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†åˆ«å¤„ç†
        for class_id in [1, 2]:  # æŒ‡é’ˆå’Œåˆ»åº¦
            if class_id not in mask:
                continue
                
            # æå–å½“å‰ç±»åˆ«çš„æ©ç 
            class_mask = (mask == class_id).astype(np.uint8)
            
            if np.sum(class_mask) == 0:
                continue
            
            # 1. å»é™¤å°çš„ç¦»ç¾¤ç‚¹ - å¼€è¿ç®—ï¼ˆå…ˆè…èš€åè†¨èƒ€ï¼‰
            if config['remove_noise']:
                kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_OPEN, kernel_small)
            
            # 2. è¿é€šåŸŸåˆ†æï¼Œä¿ç•™æœ€å¤§çš„è¿é€šåŒºåŸŸ
            if config['keep_largest_component']:
                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(class_mask, connectivity=8)
                
                if num_labels > 1:  # æœ‰è¿é€šåŸŸï¼ˆé™¤äº†èƒŒæ™¯ï¼‰
                    # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåŸŸï¼ˆæ’é™¤èƒŒæ™¯æ ‡ç­¾0ï¼‰
                    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
                    class_mask = (labels == largest_label).astype(np.uint8)
            
            # 3. æ ¹æ®ç±»åˆ«è¿›è¡Œç‰¹å®šå¤„ç†
            if class_id == 1:  # æŒ‡é’ˆ
                # æŒ‡é’ˆéœ€è¦ç»†åŒ–ï¼Œä½¿ç”¨è¾ƒå°çš„è…èš€æ ¸
                if config['pointer_erosion'] > 0:
                    kernel_pointer = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                    class_mask = cv2.erode(class_mask, kernel_pointer, iterations=config['pointer_erosion'])
                
            elif class_id == 2:  # åˆ»åº¦
                # åˆ»åº¦éœ€è¦æ›´å¤šè…èš€æ¥æ”¶ç¼©è¾¹ç•Œï¼Œé˜²æ­¢å¤–ç§»
                if config['scale_erosion'] > 0:
                    kernel_scale = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                    class_mask = cv2.erode(class_mask, kernel_scale, iterations=config['scale_erosion'])
                
                # å¯¹åˆ»åº¦è¿›è¡Œé¢å¤–çš„å½¢æ€å­¦é—­è¿ç®—ï¼Œè¿æ¥æ–­è£‚çš„åˆ»åº¦çº¿
                if config['connect_scale_lines']:
                    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                    class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel_close)
            
            # 4. å¡«å……å°æ´
            if config['fill_holes']:
                kernel_fill = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
                class_mask = cv2.morphologyEx(class_mask, cv2.MORPH_CLOSE, kernel_fill)
            
            # å°†å¤„ç†åçš„ç±»åˆ«æ©ç æ”¾å›åŸæ©ç 
            processed_mask[mask == class_id] = 0  # å…ˆæ¸…é™¤åŸæ¥çš„
            processed_mask[class_mask == 1] = class_id  # å†æ”¾å…¥å¤„ç†åçš„
        
        return processed_mask
    
    def segment_meter(self, image: np.ndarray) -> np.ndarray:
        """
        Segment meter image into classes using ONNX
        
        Args:
            image: Input meter image (BGR format)
            
        Returns:
            Segmentation mask with class labels
        """
        if self.session is None:
            print("âš ï¸  No ONNX model loaded, returning dummy mask")
            # Return a dummy mask with some basic segmentation
            h, w = image.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            # Create a simple circular mask as placeholder
            center = (w//2, h//2)
            radius = min(w, h) // 4
            cv2.circle(mask, center, radius, 1, -1)  # pointer region
            cv2.circle(mask, center, radius + 20, 2, 10)  # scale region
            return mask
        
        original_size = image.shape[:2]
        
        # Preprocess
        input_data = self.preprocess_image(image)
        
        # Get input name
        input_name = self.session.get_inputs()[0].name
        
        # Run inference
        try:
            outputs = self.session.run(None, {input_name: input_data})
            output = outputs[0]  # First output
            
            # Get predictions (argmax along channel dimension)
            predictions = np.argmax(output, axis=1).squeeze(0).astype(np.uint8)
            
            # Resize back to original size
            mask = cv2.resize(predictions, 
                             (original_size[1], original_size[0]), 
                             interpolation=cv2.INTER_NEAREST)
            
            # åå¤„ç†ï¼šå»é™¤ç¦»ç¾¤ç‚¹å’Œä¼˜åŒ–è¾¹ç•Œ
            mask = self.post_process_mask(mask)
            
            return mask
            
        except Exception as e:
            print(f"âŒ ONNX inference error: {e}")
            # Return dummy mask on error
            h, w = image.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            return mask


class DigitDetector:
    """YOLO-based digit detection for LCD displays"""
    
    def __init__(self, model_path: str):
        """Initialize digit detector with trained model"""
        if os.path.exists(model_path):
            self.model = YOLO(model_path)
            print(f"âœ… Loaded digit detection model: {model_path}")
        else:
            print(f"âš ï¸ Digit model not found at {model_path}, using base YOLO")
            self.model = YOLO("yolov10n.pt")
        
        # Define class names for digits
        self.class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'point']
        
    def detect_digits(self, image: np.ndarray, conf_threshold: float = 0.3) -> List[Dict]:
        """
        Detect digits in LCD display image
        
        Args:
            image: Input image (BGR format)
            conf_threshold: Confidence threshold for detection
            
        Returns:
            List of detection results with digit information
        """
        results = self.model(image, conf=conf_threshold)
        
        detections = []
        for r in results:
            boxes = r.boxes
            if boxes is not None:
                for i in range(len(boxes)):
                    x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy()
                    conf = boxes.conf[i].cpu().numpy()
                    cls = int(boxes.cls[i].cpu().numpy())
                    
                    # Get class name
                    if hasattr(r, 'names') and cls in r.names:
                        class_name = r.names[cls]
                    elif cls < len(self.class_names):
                        class_name = self.class_names[cls]
                    else:
                        class_name = str(cls)
                    
                    detections.append({
                        'bbox': [int(x1), int(y1), int(x2), int(y2)],
                        'confidence': float(conf),
                        'class_id': cls,
                        'class': class_name,
                        'center_x': (x1 + x2) / 2,
                        'center_y': (y1 + y2) / 2
                    })
        
        return detections
    
    def filter_duplicate_detections(self, detections: List[Dict], 
                                  overlap_threshold: float = 0.7,
                                  distance_threshold: float = 30) -> List[Dict]:
        """
        Filter out duplicate detections that are too close to each other
        
        Args:
            detections: List of detection results
            overlap_threshold: IoU threshold for considering detections as duplicates
            distance_threshold: Distance threshold in pixels
            
        Returns:
            Filtered detections list
        """
        if len(detections) <= 1:
            return detections
        
        # Sort by confidence (highest first)
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        filtered = []
        
        for det in detections:
            is_duplicate = False
            
            for existing in filtered:
                # å¯¹äºç›¸åŒçš„æ•°å­—ç±»åˆ«ï¼Œæ£€æŸ¥æ˜¯å¦è¿‡äºæ¥è¿‘
                if det['class'] == existing['class']:
                    # å°æ•°ç‚¹å•ç‹¬å¤„ç†ï¼Œå…è®¸å¤šä¸ªå°æ•°ç‚¹å€™é€‰ï¼Œåç»­ä¼šåˆå¹¶
                    if det['class'] == 'point':
                        # å°æ•°ç‚¹çš„è·ç¦»é˜ˆå€¼æ›´ä¸¥æ ¼
                        dx = det['center_x'] - existing['center_x']
                        dy = det['center_y'] - existing['center_y']
                        distance = np.sqrt(dx*dx + dy*dy)
                        
                        # å°æ•°ç‚¹è·ç¦»å¾ˆè¿‘æ‰è®¤ä¸ºæ˜¯é‡å¤
                        if distance < distance_threshold * 0.5:  # æ›´ä¸¥æ ¼çš„è·ç¦»è¦æ±‚
                            is_duplicate = True
                            break
                    else:
                        # å¯¹äºæ•°å­—ï¼Œæ£€æŸ¥è·ç¦»å’Œé‡å 
                        dx = det['center_x'] - existing['center_x']
                        dy = det['center_y'] - existing['center_y']
                        distance = np.sqrt(dx*dx + dy*dy)
                        
                        # Calculate IoU
                        iou = self._calculate_iou(det['bbox'], existing['bbox'])
                        
                        # å¦‚æœè·ç¦»å¾ˆè¿‘æˆ–é‡å å¾ˆå¤§ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                        if distance < distance_threshold or iou > overlap_threshold:
                            is_duplicate = True
                            break
                
                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ˜¯ä¸åŒæ•°å­—ä½†ä½ç½®å‡ ä¹é‡å ï¼Œå¯èƒ½æ˜¯è¯¯è¯†åˆ«
                elif det['class'] != 'point' and existing['class'] != 'point':
                    iou = self._calculate_iou(det['bbox'], existing['bbox'])
                    # å¦‚æœä¸¤ä¸ªä¸åŒæ•°å­—çš„é‡å åº¦å¾ˆé«˜ï¼Œä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„
                    if iou > 0.8:  # é«˜é‡å é˜ˆå€¼
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                filtered.append(det)
        
        return filtered
    
    def _calculate_iou(self, box1: List[int], box2: List[int]) -> float:
        """Calculate Intersection over Union (IoU) of two bounding boxes"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # Calculate intersection
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Calculate union
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def extract_reading(self, detections: List[Dict]) -> str:
        """
        Extract the complete reading from detected digits with intelligent grouping
        
        Args:
            detections: List of filtered detections
            
        Returns:
            String representation of the reading
        """
        if not detections:
            return ""
        
        # æŒ‰xåæ ‡æ’åºï¼ˆä»å·¦åˆ°å³ï¼‰
        sorted_detections = sorted(detections, key=lambda x: x['center_x'])
        
        # è¿›ä¸€æ­¥å¤„ç†æ•°å­—åºåˆ—ï¼Œå¤„ç†é‡å¤å’Œåˆ†ç»„
        final_reading = self._process_digit_sequence(sorted_detections)
        
        return final_reading
    
    def _process_digit_sequence(self, sorted_detections: List[Dict]) -> str:
        """
        å¤„ç†æ•°å­—åºåˆ—ï¼Œæ™ºèƒ½åˆ†ç»„å¹¶æ„å»ºæœ€ç»ˆè¯»æ•°
        
        Args:
            sorted_detections: æŒ‰xåæ ‡æ’åºçš„æ£€æµ‹ç»“æœ
            
        Returns:
            å¤„ç†åçš„è¯»æ•°å­—ç¬¦ä¸²
        """
        if not sorted_detections:
            return ""
        
        # åˆ†ææ•°å­—çš„ä½ç½®åˆ†å¸ƒï¼Œç¡®å®šæ•°å­—ç»„
        digit_groups = self._group_digits_by_position(sorted_detections)
        
        # ä¸ºæ¯ä¸ªç»„æ„å»ºè¯»æ•°
        readings = []
        for group in digit_groups:
            group_reading = self._construct_group_reading(group)
            if group_reading:
                readings.append(group_reading)
        
        # åˆå¹¶æ‰€æœ‰è¯»æ•°ï¼ˆç”¨ç©ºæ ¼åˆ†éš”å¤šä¸ªç‹¬ç«‹çš„æ•°å­—ï¼‰
        if len(readings) == 1:
            return readings[0]
        else:
            return " ".join(readings)
    
    def _group_digits_by_position(self, detections: List[Dict]) -> List[List[Dict]]:
        """
        æ ¹æ®ä½ç½®å°†æ•°å­—åˆ†ç»„ï¼Œè¯†åˆ«ä¸åŒçš„æ•°å­—æ˜¾ç¤ºåŒºåŸŸ
        
        Args:
            detections: æ’åºåçš„æ£€æµ‹ç»“æœ
            
        Returns:
            æ•°å­—ç»„åˆ—è¡¨
        """
        if not detections:
            return []
        
        groups = []
        current_group = [detections[0]]
        
        # è®¡ç®—å¹³å‡å­—ç¬¦å®½åº¦å’Œé—´è·ä½œä¸ºåˆ†ç»„ä¾æ®
        widths = [det['bbox'][2] - det['bbox'][0] for det in detections]
        avg_width = np.mean(widths) if widths else 50
        
        for i in range(1, len(detections)):
            prev_det = detections[i-1]
            curr_det = detections[i]
            
            # è®¡ç®—ä¸¤ä¸ªæ£€æµ‹æ¡†ä¹‹é—´çš„é—´è·
            prev_right = prev_det['bbox'][2]
            curr_left = curr_det['bbox'][0]
            gap = curr_left - prev_right
            
            # å¦‚æœé—´è·å¤§äºå¹³å‡å®½åº¦çš„1.5å€ï¼Œè®¤ä¸ºæ˜¯æ–°çš„æ•°å­—ç»„
            # æˆ–è€…å¦‚æœæ˜¯å°æ•°ç‚¹ï¼Œé—´è·æ›´å°ä¹Ÿå¯èƒ½æ˜¯æ–°ç»„
            gap_threshold = avg_width * 1.5
            if prev_det['class'] == 'point' or curr_det['class'] == 'point':
                gap_threshold = avg_width * 0.8  # å°æ•°ç‚¹é™„è¿‘çš„é˜ˆå€¼æ›´å°
            
            if gap > gap_threshold:
                groups.append(current_group)
                current_group = [curr_det]
            else:
                current_group.append(curr_det)
        
        groups.append(current_group)
        return groups
    
    def _construct_group_reading(self, group: List[Dict]) -> str:
        """
        ä¸ºå•ä¸ªæ•°å­—ç»„æ„å»ºè¯»æ•°ï¼Œå¤„ç†é‡å¤æ•°å­—
        
        Args:
            group: å•ä¸ªç»„çš„æ£€æµ‹ç»“æœ
            
        Returns:
            è¯¥ç»„çš„è¯»æ•°å­—ç¬¦ä¸²
        """
        if not group:
            return ""
        
        # å†æ¬¡æŒ‰xåæ ‡ç²¾ç¡®æ’åº
        group = sorted(group, key=lambda x: x['center_x'])
        
        # å¤„ç†ä½ç½®ç›¸è¿‘çš„é‡å¤æ•°å­—
        filtered_group = self._remove_positional_duplicates(group)
        
        # æ„å»ºè¯»æ•°å­—ç¬¦ä¸²
        reading_parts = []
        for det in filtered_group:
            if det['class'] == 'point':
                reading_parts.append(".")
            else:
                reading_parts.append(str(det['class']))
        
        reading = "".join(reading_parts)
        
        # æ¸…ç†è¯»æ•°
        reading = self._validate_reading(reading)
        
        return reading
    
    def _remove_positional_duplicates(self, group: List[Dict]) -> List[Dict]:
        """
        ç§»é™¤ä½ç½®ç›¸è¿‘çš„é‡å¤æ•°å­—ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        
        Args:
            group: æ•°å­—ç»„
            
        Returns:
            å»é‡åçš„æ•°å­—ç»„
        """
        if len(group) <= 1:
            return group
        
        # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
        group = sorted(group, key=lambda x: x['confidence'], reverse=True)
        
        filtered = []
        for det in group:
            is_duplicate = False
            
            for existing in filtered:
                # æ£€æŸ¥æ˜¯å¦ä¸ºä½ç½®ç›¸è¿‘çš„ç›¸åŒç±»åˆ«
                if det['class'] == existing['class']:
                    # è®¡ç®—ä¸­å¿ƒè·ç¦»
                    dx = det['center_x'] - existing['center_x']
                    dy = det['center_y'] - existing['center_y']
                    distance = np.sqrt(dx*dx + dy*dy)
                    
                    # è®¡ç®—å¹³å‡æ¡†å¤§å°ä½œä¸ºè·ç¦»é˜ˆå€¼
                    avg_size = np.mean([
                        det['bbox'][2] - det['bbox'][0],
                        det['bbox'][3] - det['bbox'][1],
                        existing['bbox'][2] - existing['bbox'][0],
                        existing['bbox'][3] - existing['bbox'][1]
                    ])
                    
                    # å¦‚æœè·ç¦»å°äºå¹³å‡å°ºå¯¸çš„0.5å€ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                    if distance < avg_size * 0.5:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                filtered.append(det)
        
        # é‡æ–°æŒ‰xåæ ‡æ’åº
        return sorted(filtered, key=lambda x: x['center_x'])
    
    def _validate_reading(self, reading: str) -> str:
        """
        éªŒè¯å¹¶æ¸…ç†è¯»æ•°å­—ç¬¦ä¸²
        
        Args:
            reading: åŸå§‹è¯»æ•°å­—ç¬¦ä¸²
            
        Returns:
            æ¸…ç†åçš„è¯»æ•°å­—ç¬¦ä¸²
        """
        if not reading:
            return "0"
        
        # ç§»é™¤è¿ç»­çš„å°æ•°ç‚¹
        while '..' in reading:
            reading = reading.replace('..', '.')
        
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„å°æ•°ç‚¹
        reading = reading.strip('.')
        
        # å¦‚æœä¸ºç©ºï¼Œè¿”å›"0"
        if not reading:
            return "0"
        
        # å¤„ç†å¤šä¸ªå°æ•°ç‚¹çš„æƒ…å†µï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
        if reading.count('.') > 1:
            parts = reading.split('.')
            if parts[0]:  # å¦‚æœç¬¬ä¸€éƒ¨åˆ†ä¸ä¸ºç©º
                reading = parts[0] + '.' + ''.join(parts[1:])
            else:  # å¦‚æœç¬¬ä¸€éƒ¨åˆ†ä¸ºç©ºï¼Œå–ç¬¬äºŒéƒ¨åˆ†ä½œä¸ºæ•´æ•°
                reading = ''.join(parts[1:])
                if '.' in reading:  # å¦‚æœè¿˜æœ‰å°æ•°ç‚¹
                    sub_parts = reading.split('.')
                    reading = sub_parts[0] + '.' + ''.join(sub_parts[1:])
        
        # ç¡®ä¿ä¸ä»¥å°æ•°ç‚¹å¼€å¤´
        if reading.startswith('.'):
            reading = '0' + reading
        
        # ç§»é™¤æœ«å°¾çš„å°æ•°ç‚¹
        if reading.endswith('.'):
            reading = reading[:-1]
        
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—æ ¼å¼
        try:
            float(reading)
        except ValueError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆæ•°å­—ï¼Œå°è¯•æå–çº¯æ•°å­—
            digits_only = ''.join([c for c in reading if c.isdigit() or c == '.'])
            if digits_only and digits_only != '.':
                reading = digits_only
                # å†æ¬¡æ¸…ç†
                if reading.count('.') > 1:
                    parts = reading.split('.')
                    reading = parts[0] + '.' + ''.join(parts[1:])
            else:
                reading = "0"
        
        return reading
    
    def visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """
        Visualize digit detections on image
        
        Args:
            image: Input image
            detections: List of detections
            
        Returns:
            Image with visualization
        """
        vis_img = image.copy()
        
        # Define colors for different classes
        colors = {
            'point': (0, 0, 255),  # Red for decimal point
            'digit': (0, 255, 0)   # Green for digits
        }
        
        for det in detections:
            bbox = det['bbox']
            conf = det['confidence']
            class_name = det['class']
            
            # Choose color
            color = colors['point'] if class_name == 'point' else colors['digit']
            
            # Draw bounding box
            cv2.rectangle(vis_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
            
            # Draw label
            label = f"{class_name}: {conf:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            
            # Background for label
            cv2.rectangle(vis_img, 
                         (bbox[0], bbox[1] - label_size[1] - 10), 
                         (bbox[0] + label_size[0], bbox[1]), 
                         color, -1)
            
            # Label text
            cv2.putText(vis_img, label, 
                       (bbox[0], bbox[1] - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            
            # Draw center point
            center = (int(det['center_x']), int(det['center_y']))
            cv2.circle(vis_img, center, 3, color, -1)
        
        return vis_img


class MeterReadingApp:
    """Complete meter reading application"""
    
    def __init__(self):
        """Initialize the application"""
        # Model paths
        self.detection_model_path = "models/detection/detection_model.pt"
        self.segmentation_model_path = "models/segmentation/segmentation_model.onnx"
        
        # Fallback to base models if trained models not available
        if not os.path.exists(self.detection_model_path):
            self.detection_model_path = "yolov10n.pt"
            print("Using base YOLOv10 model (not trained on meters)")
        
        # Initialize components
        self.detector = MeterDetector(self.detection_model_path)
        
        # Determine device
        self.device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        
        self.segmentor = MeterSegmentor(self.segmentation_model_path, self.device)
        self.reader = MeterReader(scale_range=(0.0, 1.6), debug=False)
    
    def process_image(self, image: np.ndarray, conf_threshold: float = 0.5, 
                     scale_min: float = 0.0, scale_max: float = 1.6) -> Dict[str, Any]:
        """
        Complete processing pipeline for meter reading
        
        Args:
            image: Input image
            conf_threshold: Detection confidence threshold
            scale_min: Minimum scale value
            scale_max: Maximum scale value
            
        Returns:
            Dictionary with all results and visualizations
        """
        results = {
            'success': False,
            'error': None,
            'detections': [],
            'readings': [],
            'visualizations': {}
        }
        
        try:
            # Step 1: Detection
            detections = self.detector.detect_meters(image, conf_threshold)
            results['detections'] = detections
            
            if not detections:
                results['error'] = "No meters detected in the image"
                return results
            
            # Process each detected meter
            for i, detection in enumerate(detections):
                try:
                    # Step 2: Crop meter region
                    cropped_meter = self.detector.crop_meter(image, detection['bbox'])
                    
                    # Step 3: Segmentation
                    segmentation_mask = self.segmentor.segment_meter(cropped_meter)
                    
                    # Step 4: Reading extraction
                    self.reader.scale_beginning = scale_min
                    self.reader.scale_end = scale_max
                    reading = self.reader.process_single_meter(cropped_meter, segmentation_mask)
                    
                    if reading is not None:
                        results['readings'].append({
                            'meter_id': i,
                            'reading': reading,
                            'confidence': detection['confidence'],
                            'bbox': detection['bbox']
                        })
                    
                    # Generate visualizations
                    vis_detection = self._visualize_detection(image, [detection])
                    vis_crop = cropped_meter
                    vis_segmentation = self._visualize_segmentation(cropped_meter, segmentation_mask)
                    vis_result = self._visualize_reading_result(cropped_meter, segmentation_mask, reading)
                    
                    results['visualizations'][f'meter_{i}'] = {
                        'detection': vis_detection,
                        'crop': vis_crop,
                        'segmentation': vis_segmentation,
                        'result': vis_result
                    }
                    
                except Exception as e:
                    print(f"Error processing meter {i}: {e}")
                    continue
            
            results['success'] = len(results['readings']) > 0
            if not results['success']:
                results['error'] = "Failed to extract readings from detected meters"
                
        except Exception as e:
            results['error'] = f"Processing error: {str(e)}"
            
        return results
    
    def _visualize_detection(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """Visualize detection results"""
        vis_img = image.copy()
        
        for detection in detections:
            bbox = detection['bbox']
            conf = detection['confidence']
            
            # Draw bounding box
            cv2.rectangle(vis_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
            
            # Draw label
            label = f"Meter: {conf:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(vis_img, (bbox[0], bbox[1] - label_size[1] - 10), 
                         (bbox[0] + label_size[0], bbox[1]), (0, 255, 0), -1)
            cv2.putText(vis_img, label, (bbox[0], bbox[1] - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        return vis_img
    
    def _visualize_segmentation(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Visualize segmentation results"""
        # Create colored mask
        colored_mask = np.zeros_like(image)
        colored_mask[mask == 1] = [0, 0, 255]    # Pointer - Red
        colored_mask[mask == 2] = [0, 255, 0]    # Scale - Green
        
        # Blend with original image
        alpha = 0.6
        vis_img = cv2.addWeighted(image, 1-alpha, colored_mask, alpha, 0)
        
        # Add statistics text
        pointer_pixels = np.sum(mask == 1)
        scale_pixels = np.sum(mask == 2)
        total_pixels = mask.shape[0] * mask.shape[1]
        
        # Add text overlay with statistics
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.4
        thickness = 1
        
        text_lines = [
            f"Pointer: {pointer_pixels} px ({pointer_pixels/total_pixels*100:.1f}%)",
            f"Scale: {scale_pixels} px ({scale_pixels/total_pixels*100:.1f}%)",
            f"Post-processed: Cleaned noise & boundaries"
        ]
        
        y_offset = 15
        for i, text in enumerate(text_lines):
            y_pos = y_offset + i * 15
            # Add background rectangle for better readability
            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
            cv2.rectangle(vis_img, (5, y_pos - 12), (text_size[0] + 10, y_pos + 3), (0, 0, 0), -1)
            cv2.putText(vis_img, text, (8, y_pos), font, font_scale, (255, 255, 255), thickness)
        
        return vis_img
    
    def _visualize_reading_result(self, image: np.ndarray, mask: np.ndarray, reading: Optional[float]) -> np.ndarray:
        """Visualize final reading result"""
        if reading is None:
            return image
        
        # Use the reader's visualization method
        try:
            # Extract components for visualization
            pointer_mask = self.reader.threshold_by_category(mask, 1)
            scale_mask = self.reader.threshold_by_category(mask, 2)
            
            # Find components
            scale_locations = self.reader.get_scale_locations(scale_mask)
            center = self.reader.get_center_location(image)
            pointer_locations = self.reader.get_pointer_locations(pointer_mask, center) if center else None
            
            if all([scale_locations, center, pointer_locations]):
                vis_img = self.reader.visualize_result(image, scale_locations, pointer_locations, center, reading)
                return vis_img
        except:
            pass
        
        # Fallback: simple text overlay
        vis_img = image.copy()
        text = f"Reading: {reading:.3f}"
        cv2.putText(vis_img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        return vis_img


class DigitReadingApp:
    """LCD digit reading application"""
    
    def __init__(self):
        """Initialize the digit reading application"""
        # Model path for digit detection
        self.digit_model_path = "models/detection/digits_model.pt"
        
        # Initialize digit detector
        self.digit_detector = DigitDetector(self.digit_model_path)
        print(f"Digit reading app initialized with model: {self.digit_model_path}")
    
    def process_digit_image(self, image: np.ndarray, conf_threshold: float = 0.3,
                          overlap_threshold: float = 0.7, 
                          distance_threshold: float = 30) -> Dict[str, Any]:
        """
        Process LCD digit image and extract reading
        
        Args:
            image: Input image (BGR format)
            conf_threshold: Detection confidence threshold
            overlap_threshold: IoU threshold for duplicate filtering
            distance_threshold: Distance threshold for duplicate filtering
            
        Returns:
            Dictionary with processing results
        """
        results = {
            'success': False,
            'reading': '',
            'raw_detections': [],
            'filtered_detections': [],
            'error': None
        }
        
        try:
            # Step 1: Detect all digits
            raw_detections = self.digit_detector.detect_digits(image, conf_threshold)
            results['raw_detections'] = raw_detections
            
            if not raw_detections:
                results['error'] = "No digits detected in the image"
                return results
            
            # Step 2: Filter duplicate detections
            filtered_detections = self.digit_detector.filter_duplicate_detections(
                raw_detections, overlap_threshold, distance_threshold)
            results['filtered_detections'] = filtered_detections
            
            # Step 3: Extract reading
            reading = self.digit_detector.extract_reading(filtered_detections)
            results['reading'] = reading
            
            results['success'] = True
            
        except Exception as e:
            results['error'] = f"Processing error: {str(e)}"
        
        return results


def create_gradio_interface():
    """Create Gradio interface with tabs for different functionalities"""
    
    # Initialize apps
    meter_app = MeterReadingApp()
    digit_app = DigitReadingApp()
    
    def process_uploaded_image(image, conf_threshold, scale_min, scale_max):
        """Process uploaded image and return results for meter reading"""
        if image is None:
            return None, None, None, None, "Please upload an image"
        
        # Convert PIL to OpenCV format
        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # Process image
        results = meter_app.process_image(image_cv, conf_threshold, scale_min, scale_max)
        
        if not results['success']:
            error_msg = results.get('error', 'Unknown error occurred')
            return None, None, None, None, error_msg
        
        # Prepare outputs
        summary_text = f"Found {len(results['readings'])} meter(s)\n\n"
        for reading in results['readings']:
            summary_text += f"Meter {reading['meter_id']}: {reading['reading']:.3f} (conf: {reading['confidence']:.2f})\n"
        
        # Get visualizations for first meter
        if results['visualizations']:
            first_meter = list(results['visualizations'].keys())[0]
            vis = results['visualizations'][first_meter]
            
            # Convert BGR to RGB for display
            detection_img = cv2.cvtColor(vis['detection'], cv2.COLOR_BGR2RGB)
            crop_img = cv2.cvtColor(vis['crop'], cv2.COLOR_BGR2RGB)
            segmentation_img = cv2.cvtColor(vis['segmentation'], cv2.COLOR_BGR2RGB)
            result_img = cv2.cvtColor(vis['result'], cv2.COLOR_BGR2RGB)
            
            return detection_img, crop_img, segmentation_img, result_img, summary_text
        
        return None, None, None, None, summary_text
    
    def process_digit_image(image, conf_threshold, overlap_threshold, distance_threshold):
        """Process LCD digit image and return results"""
        if image is None:
            return None, None, "Please upload an image"
        
        # Convert PIL to OpenCV format
        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # Process image
        results = digit_app.process_digit_image(image_cv, conf_threshold, 
                                              overlap_threshold, distance_threshold)
        
        if not results['success']:
            error_msg = results.get('error', 'Unknown error occurred')
            return None, None, error_msg
        
        # Prepare visualizations
        raw_vis = digit_app.digit_detector.visualize_detections(
            image_cv, results['raw_detections'])
        filtered_vis = digit_app.digit_detector.visualize_detections(
            image_cv, results['filtered_detections'])
        
        # Convert BGR to RGB for display
        raw_vis_rgb = cv2.cvtColor(raw_vis, cv2.COLOR_BGR2RGB)
        filtered_vis_rgb = cv2.cvtColor(filtered_vis, cv2.COLOR_BGR2RGB)
        
        # åˆ›å»ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯
        raw_count = len(results['raw_detections'])
        filtered_count = len(results['filtered_detections'])
        removed_count = raw_count - filtered_count
        
        # åˆ†ææ£€æµ‹åˆ°çš„æ•°å­—ç±»åˆ«
        detected_classes = {}
        for det in results['filtered_detections']:
            class_name = det['class']
            if class_name not in detected_classes:
                detected_classes[class_name] = 0
            detected_classes[class_name] += 1
        
        class_info = ", ".join([f"{k}: {v}" for k, v in detected_classes.items()])
        
        # æ„å»ºè¯¦ç»†ä¿¡æ¯
        summary_text = f"ğŸ“± LCDè¯»æ•°è¯†åˆ«ç»“æœ\n"
        summary_text += f"{'='*40}\n\n"
        
        summary_text += f"âœ… æœ€ç»ˆè¯»æ•°: **{results['reading']}**\n\n"
        
        summary_text += f"ğŸ“Š æ£€æµ‹ç»Ÿè®¡:\n"
        summary_text += f"  ğŸ” åŸå§‹æ£€æµ‹: {raw_count} ä¸ªæ•°å­—\n"
        summary_text += f"  ğŸ¯ è¿‡æ»¤å: {filtered_count} ä¸ªæ•°å­—\n"
        summary_text += f"  ğŸ—‘ï¸ ç§»é™¤é‡å¤: {removed_count} ä¸ª\n\n"
        
        if class_info:
            summary_text += f"ğŸ“‹ æ£€æµ‹ç±»åˆ«ç»Ÿè®¡:\n"
            summary_text += f"  {class_info}\n\n"
        
        summary_text += f"âš™ï¸ å¤„ç†å‚æ•°:\n"
        summary_text += f"  ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}\n"
        summary_text += f"  é‡å é˜ˆå€¼: {overlap_threshold}\n"
        summary_text += f"  è·ç¦»é˜ˆå€¼: {distance_threshold} åƒç´ \n\n"
        
        if results['filtered_detections']:
            summary_text += "ğŸ” æœ€ç»ˆæ£€æµ‹è¯¦æƒ… (ä»å·¦åˆ°å³):\n"
            sorted_dets = sorted(results['filtered_detections'], key=lambda x: x['center_x'])
            for i, det in enumerate(sorted_dets):
                pos_x = int(det['center_x'])
                pos_y = int(det['center_y'])
                summary_text += f"  {i+1}. '{det['class']}' (ç½®ä¿¡åº¦: {det['confidence']:.3f}, ä½ç½®: {pos_x},{pos_y})\n"
        
        # å¦‚æœæœ‰è¢«ç§»é™¤çš„æ£€æµ‹ï¼Œæ˜¾ç¤ºä¸€äº›ä¿¡æ¯
        if removed_count > 0:
            summary_text += f"\nâš ï¸ ç§»é™¤çš„é‡å¤æ£€æµ‹:\n"
            removed_dets = [det for det in results['raw_detections'] 
                          if det not in results['filtered_detections']]
            for i, det in enumerate(removed_dets[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                summary_text += f"  - '{det['class']}' (ç½®ä¿¡åº¦: {det['confidence']:.3f})\n"
            if len(removed_dets) > 5:
                summary_text += f"  ... ä»¥åŠå…¶ä»– {len(removed_dets)-5} ä¸ªé‡å¤æ£€æµ‹\n"
        
        return raw_vis_rgb, filtered_vis_rgb, summary_text
    
    # Create interface with tabs
    with gr.Blocks(title="Meter Reading Extraction", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ”§ Industrial Meter & LCD Display Reading System
        
        AI-powered system for extracting readings from industrial meters and LCD displays.
        """)
        
        with gr.Tabs():
            # Tab 1: Traditional Meter Reading
            with gr.TabItem("ğŸ”§ Industrial Meters"):
                gr.Markdown("""
                ## Industrial Meter Reading Extraction
                
                Upload an image containing industrial meters to automatically extract readings using AI.
                
                **Pipeline:** Detection â†’ Cropping â†’ Segmentation â†’ Reading Extraction
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        # Input section
                        gr.Markdown("### ğŸ“¤ Input")
                        meter_image_input = gr.Image(type="pil", label="Upload Meter Image")
                        
                        with gr.Row():
                            meter_conf_threshold = gr.Slider(0.1, 0.9, value=0.5, step=0.1, 
                                                           label="Detection Confidence")
                            
                        with gr.Row():
                            scale_min = gr.Number(value=0.0, label="Scale Min Value")
                            scale_max = gr.Number(value=1.6, label="Scale Max Value")
                        
                        meter_process_btn = gr.Button("ğŸš€ Extract Readings", variant="primary", size="lg")
                        
                        # Results summary
                        gr.Markdown("### ğŸ“Š Results")
                        meter_results_text = gr.Textbox(label="Summary", lines=5, interactive=False)
                    
                    with gr.Column(scale=2):
                        # Visualization section
                        gr.Markdown("### ğŸ‘ï¸ Process Visualization")
                        
                        with gr.Row():
                            meter_detection_output = gr.Image(label="1. Detection Results")
                            meter_crop_output = gr.Image(label="2. Cropped Meter")
                        
                        with gr.Row():
                            meter_segmentation_output = gr.Image(label="3. Segmentation Masks")
                            meter_result_output = gr.Image(label="4. Final Reading")
                
                # Event handlers for meter tab
                meter_process_btn.click(
                    fn=process_uploaded_image,
                    inputs=[meter_image_input, meter_conf_threshold, scale_min, scale_max],
                    outputs=[meter_detection_output, meter_crop_output, 
                           meter_segmentation_output, meter_result_output, meter_results_text]
                )
                
                # Examples for meter
                gr.Markdown("### ğŸ“‹ Usage Instructions")
                gr.Markdown("""
                1. **Upload Image**: Choose an image containing industrial meters
                2. **Adjust Settings**: 
                   - Detection Confidence: Higher values = more strict detection
                   - Scale Range: Set the min/max values of your meter scale
                3. **Process**: Click "Extract Readings" to run the complete pipeline
                4. **View Results**: Check the visualization and summary
                
                **Supported Formats**: JPG, PNG, BMP
                **Best Results**: Clear, well-lit images with visible meter faces
                """)
            
            # Tab 2: LCD Digit Reading
            with gr.TabItem("ğŸ“± LCD Display Reading"):
                gr.Markdown("""
                ## LCD Digital Display Reading
                
                Upload an image containing LCD digital displays to automatically extract numeric readings.
                
                **Features:** 
                - Detects digits 0-9 and decimal points
                - Filters duplicate detections
                - Reads from left to right (high to low digit)
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        # Input section
                        gr.Markdown("### ğŸ“¤ Input")
                        digit_image_input = gr.Image(type="pil", label="Upload LCD Display Image")
                        
                        gr.Markdown("### âš™ï¸ Detection Settings")
                        digit_conf_threshold = gr.Slider(0.1, 0.9, value=0.3, step=0.05, 
                                                       label="Detection Confidence")
                        
                        gr.Markdown("### ğŸ”§ Duplicate Filtering")
                        overlap_threshold = gr.Slider(0.1, 1.0, value=0.7, step=0.05,
                                                    label="Overlap Threshold (IoU)")
                        distance_threshold = gr.Slider(5, 100, value=30, step=5,
                                                     label="Distance Threshold (pixels)")
                        
                        digit_process_btn = gr.Button("ğŸ” Extract Reading", variant="primary", size="lg")
                        
                        # Results summary
                        gr.Markdown("### ğŸ“Š Results")
                        digit_results_text = gr.Textbox(label="Detection Summary", lines=8, interactive=False)
                    
                    with gr.Column(scale=2):
                        # Visualization section
                        gr.Markdown("### ğŸ‘ï¸ Detection Visualization")
                        
                        with gr.Row():
                            raw_detection_output = gr.Image(label="Raw Detections")
                            filtered_detection_output = gr.Image(label="Filtered Detections")
                
                # Event handlers for digit tab
                digit_process_btn.click(
                    fn=process_digit_image,
                    inputs=[digit_image_input, digit_conf_threshold, overlap_threshold, distance_threshold],
                    outputs=[raw_detection_output, filtered_detection_output, digit_results_text]
                )
                
                # Examples for digit reading
                gr.Markdown("### ğŸ“‹ Usage Instructions")
                gr.Markdown("""
                1. **Upload Image**: Choose an image containing LCD digital displays
                2. **Adjust Detection Settings**: 
                   - Detection Confidence: Lower values detect more digits but may include false positives
                3. **Configure Duplicate Filtering**:
                   - Overlap Threshold: Higher values are more strict about overlapping detections
                   - Distance Threshold: Minimum distance between same digits to be considered separate
                4. **Process**: Click "Extract Reading" to analyze the display
                5. **View Results**: Check both raw and filtered detections
                
                **Tips**:
                - For clear displays: Use higher confidence (0.5-0.7)
                - For blurry/poor quality: Use lower confidence (0.2-0.4)
                - Adjust distance threshold based on digit spacing in your displays
                
                **Supported**: Numbers 0-9, decimal points, multi-digit readings
                """)
    
    return interface


def main():
    """Main function to launch the application"""
    print("Initializing Meter Reading Extraction App...")
    
    # Create and launch interface
    interface = create_gradio_interface()
    
    print("Launching Gradio interface...")
    interface.launch(
        server_name="0.0.0.0",  # Allow external access
        server_port=7860,       # Default Gradio port
        share=False,            # Set to True for public sharing
        debug=True
    )


if __name__ == "__main__":
    main() 
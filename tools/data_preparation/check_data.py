#!/usr/bin/env python3
"""
æ•°æ®é›†éªŒè¯è„šæœ¬

æ£€æŸ¥COCOæ ¼å¼æ•°æ®é›†çš„å®Œæ•´æ€§ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡ä»¶ã€æ ‡æ³¨æ–‡ä»¶ã€æ•°æ®æ ¼å¼ç­‰ã€‚

ä½œè€…: chijiang
æ—¥æœŸ: 2025-06-06
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from collections import Counter, defaultdict

class DatasetValidator:
    """æ•°æ®é›†éªŒè¯å™¨"""
    
    def __init__(self, data_root: str):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•
        """
        self.data_root = Path(data_root)
        self.issues = []
        
    def validate_detection_dataset(self) -> Dict:
        """
        éªŒè¯æ£€æµ‹æ•°æ®é›†
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        print("ğŸ” Validating detection dataset...")
        
        results = {
            'train': self._validate_split('train'),
            'val': self._validate_split('val'),
            'summary': {}
        }
        
        # æ±‡æ€»ç»Ÿè®¡
        total_images = results['train']['num_images'] + results['val']['num_images']
        total_annotations = results['train']['num_annotations'] + results['val']['num_annotations']
        
        results['summary'] = {
            'total_images': total_images,
            'total_annotations': total_annotations,
            'avg_annotations_per_image': total_annotations / total_images if total_images > 0 else 0,
            'issues_found': len(self.issues)
        }
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        self._print_validation_results(results)
        
        return results
    
    def _validate_split(self, split: str) -> Dict:
        """éªŒè¯å•ä¸ªæ•°æ®åˆ†å‰²"""
        print(f"ğŸ“Š Validating {split} split...")
        
        # æ–‡ä»¶è·¯å¾„
        if split == 'train':
            image_dir = self.data_root / "train2017"
            ann_file = self.data_root / "annotations" / "meter_coco_train.json"
        else:
            image_dir = self.data_root / "val2017"
            ann_file = self.data_root / "annotations" / "meter_coco_val.json"
        
        # æ£€æŸ¥ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨æ€§
        if not image_dir.exists():
            self.issues.append(f"Image directory not found: {image_dir}")
            return {'error': f"Image directory not found: {image_dir}"}
        
        if not ann_file.exists():
            self.issues.append(f"Annotation file not found: {ann_file}")
            return {'error': f"Annotation file not found: {ann_file}"}
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        try:
            with open(ann_file, 'r') as f:
                coco_data = json.load(f)
        except Exception as e:
            self.issues.append(f"Failed to load annotation file {ann_file}: {str(e)}")
            return {'error': f"Failed to load annotation file: {str(e)}"}
        
        # éªŒè¯COCOæ ¼å¼
        validation_result = self._validate_coco_format(coco_data, image_dir, split)
        
        # ç»Ÿè®¡ä¿¡æ¯
        validation_result.update(self._compute_statistics(coco_data, image_dir))
        
        return validation_result
    
    def _validate_coco_format(self, coco_data: Dict, image_dir: Path, split: str) -> Dict:
        """éªŒè¯COCOæ ¼å¼æ­£ç¡®æ€§"""
        issues = []
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['images', 'annotations', 'categories']
        for field in required_fields:
            if field not in coco_data:
                issues.append(f"Missing required field '{field}' in {split} annotations")
        
        if issues:
            self.issues.extend(issues)
            return {'format_valid': False, 'issues': issues}
        
        # éªŒè¯å›¾åƒä¿¡æ¯
        image_issues = self._validate_images(coco_data['images'], image_dir)
        
        # éªŒè¯æ ‡æ³¨ä¿¡æ¯
        annotation_issues = self._validate_annotations(coco_data['annotations'], coco_data['images'])
        
        # éªŒè¯ç±»åˆ«ä¿¡æ¯
        category_issues = self._validate_categories(coco_data['categories'])
        
        all_issues = image_issues + annotation_issues + category_issues
        self.issues.extend(all_issues)
        
        return {
            'format_valid': len(all_issues) == 0,
            'issues': all_issues
        }
    
    def _validate_images(self, images: List[Dict], image_dir: Path) -> List[str]:
        """éªŒè¯å›¾åƒä¿¡æ¯"""
        issues = []
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶å­˜åœ¨æ€§
        missing_files = []
        corrupted_files = []
        size_mismatches = []
        
        for img_info in tqdm(images, desc="Validating images"):
            file_path = image_dir / img_info['file_name']
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
            if not file_path.exists():
                missing_files.append(img_info['file_name'])
                continue
            
            # æ£€æŸ¥å›¾åƒæ˜¯å¦å¯è¯»
            try:
                image = cv2.imread(str(file_path))
                if image is None:
                    corrupted_files.append(img_info['file_name'])
                    continue
                
                # æ£€æŸ¥å°ºå¯¸æ˜¯å¦åŒ¹é…
                h, w = image.shape[:2]
                if w != img_info['width'] or h != img_info['height']:
                    size_mismatches.append({
                        'file': img_info['file_name'],
                        'expected': (img_info['width'], img_info['height']),
                        'actual': (w, h)
                    })
            
            except Exception as e:
                corrupted_files.append(f"{img_info['file_name']}: {str(e)}")
        
        # è®°å½•é—®é¢˜
        if missing_files:
            issues.append(f"Missing image files: {len(missing_files)} files")
            if len(missing_files) <= 10:
                issues.extend([f"  - {f}" for f in missing_files])
            else:
                issues.extend([f"  - {f}" for f in missing_files[:10]])
                issues.append(f"  - ... and {len(missing_files) - 10} more")
        
        if corrupted_files:
            issues.append(f"Corrupted image files: {len(corrupted_files)} files")
            if len(corrupted_files) <= 5:
                issues.extend([f"  - {f}" for f in corrupted_files])
        
        if size_mismatches:
            issues.append(f"Image size mismatches: {len(size_mismatches)} files")
            for mismatch in size_mismatches[:5]:
                issues.append(f"  - {mismatch['file']}: expected {mismatch['expected']}, got {mismatch['actual']}")
        
        return issues
    
    def _validate_annotations(self, annotations: List[Dict], images: List[Dict]) -> List[str]:
        """éªŒè¯æ ‡æ³¨ä¿¡æ¯"""
        issues = []
        
        # åˆ›å»ºå›¾åƒIDåˆ°å›¾åƒä¿¡æ¯çš„æ˜ å°„
        image_id_to_info = {img['id']: img for img in images}
        
        invalid_boxes = []
        orphan_annotations = []
        
        for ann in annotations:
            # æ£€æŸ¥æ ‡æ³¨æ˜¯å¦æœ‰å¯¹åº”çš„å›¾åƒ
            if ann['image_id'] not in image_id_to_info:
                orphan_annotations.append(ann['id'])
                continue
            
            # æ£€æŸ¥è¾¹ç•Œæ¡†æ ¼å¼
            bbox = ann['bbox']
            if len(bbox) != 4:
                invalid_boxes.append(f"Annotation {ann['id']}: bbox should have 4 values, got {len(bbox)}")
                continue
            
            x, y, w, h = bbox
            
            # æ£€æŸ¥è¾¹ç•Œæ¡†å€¼çš„æœ‰æ•ˆæ€§
            if w <= 0 or h <= 0:
                invalid_boxes.append(f"Annotation {ann['id']}: invalid bbox size (w={w}, h={h})")
                continue
            
            if x < 0 or y < 0:
                invalid_boxes.append(f"Annotation {ann['id']}: negative bbox coordinates (x={x}, y={y})")
                continue
            
            # æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦è¶…å‡ºå›¾åƒèŒƒå›´
            img_info = image_id_to_info[ann['image_id']]
            if x + w > img_info['width'] or y + h > img_info['height']:
                invalid_boxes.append(f"Annotation {ann['id']}: bbox exceeds image boundaries")
        
        if invalid_boxes:
            issues.append(f"Invalid bounding boxes: {len(invalid_boxes)}")
            issues.extend(invalid_boxes[:10])  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        if orphan_annotations:
            issues.append(f"Orphan annotations (no corresponding image): {len(orphan_annotations)}")
        
        return issues
    
    def _validate_categories(self, categories: List[Dict]) -> List[str]:
        """éªŒè¯ç±»åˆ«ä¿¡æ¯"""
        issues = []
        
        if not categories:
            issues.append("No categories found")
            return issues
        
        # æ£€æŸ¥ç±»åˆ«å­—æ®µ
        for cat in categories:
            if 'id' not in cat or 'name' not in cat:
                issues.append(f"Category missing required fields: {cat}")
        
        # æ£€æŸ¥ç±»åˆ«åç§°
        expected_categories = {'meter'}
        actual_categories = {cat['name'] for cat in categories}
        
        if actual_categories != expected_categories:
            issues.append(f"Unexpected categories. Expected: {expected_categories}, Got: {actual_categories}")
        
        return issues
    
    def _compute_statistics(self, coco_data: Dict, image_dir: Path) -> Dict:
        """è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        images = coco_data['images']
        annotations = coco_data['annotations']
        
        # åŸºæœ¬ç»Ÿè®¡
        num_images = len(images)
        num_annotations = len(annotations)
        
        # å›¾åƒå°ºå¯¸åˆ†å¸ƒ
        widths = [img['width'] for img in images]
        heights = [img['height'] for img in images]
        
        # æ¯å¼ å›¾åƒçš„æ ‡æ³¨æ•°é‡
        annotations_per_image = Counter()
        bbox_areas = []
        bbox_aspect_ratios = []
        
        for ann in annotations:
            annotations_per_image[ann['image_id']] += 1
            
            bbox = ann['bbox']
            area = bbox[2] * bbox[3]  # width * height
            aspect_ratio = bbox[2] / bbox[3] if bbox[3] > 0 else 0
            
            bbox_areas.append(area)
            bbox_aspect_ratios.append(aspect_ratio)
        
        # ç»Ÿè®¡ç»“æœ
        stats = {
            'num_images': num_images,
            'num_annotations': num_annotations,
            'avg_annotations_per_image': num_annotations / num_images if num_images > 0 else 0,
            'image_sizes': {
                'width': {'min': min(widths), 'max': max(widths), 'mean': np.mean(widths)},
                'height': {'min': min(heights), 'max': max(heights), 'mean': np.mean(heights)}
            },
            'bbox_stats': {
                'areas': {
                    'min': min(bbox_areas) if bbox_areas else 0,
                    'max': max(bbox_areas) if bbox_areas else 0,
                    'mean': np.mean(bbox_areas) if bbox_areas else 0
                },
                'aspect_ratios': {
                    'min': min(bbox_aspect_ratios) if bbox_aspect_ratios else 0,
                    'max': max(bbox_aspect_ratios) if bbox_aspect_ratios else 0,
                    'mean': np.mean(bbox_aspect_ratios) if bbox_aspect_ratios else 0
                }
            },
            'annotations_per_image_dist': dict(Counter(annotations_per_image.values()))
        }
        
        return stats
    
    def _print_validation_results(self, results: Dict):
        """æ‰“å°éªŒè¯ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“‹ DATASET VALIDATION RESULTS")
        print("="*60)
        
        # æ±‡æ€»ä¿¡æ¯
        summary = results['summary']
        print(f"ğŸ“Š Summary:")
        print(f"  Total images: {summary['total_images']}")
        print(f"  Total annotations: {summary['total_annotations']}")
        print(f"  Avg annotations per image: {summary['avg_annotations_per_image']:.2f}")
        print(f"  Issues found: {summary['issues_found']}")
        
        # åˆ†å‰²ç»Ÿè®¡
        for split in ['train', 'val']:
            if split in results and 'error' not in results[split]:
                split_data = results[split]
                print(f"\nğŸ“ˆ {split.capitalize()} Split:")
                print(f"  Images: {split_data['num_images']}")
                print(f"  Annotations: {split_data['num_annotations']}")
                print(f"  Avg annotations per image: {split_data['avg_annotations_per_image']:.2f}")
                
                # å›¾åƒå°ºå¯¸
                img_sizes = split_data['image_sizes']
                print(f"  Image sizes:")
                print(f"    Width: {img_sizes['width']['min']}-{img_sizes['width']['max']} (avg: {img_sizes['width']['mean']:.1f})")
                print(f"    Height: {img_sizes['height']['min']}-{img_sizes['height']['max']} (avg: {img_sizes['height']['mean']:.1f})")
                
                # è¾¹ç•Œæ¡†ç»Ÿè®¡
                bbox_stats = split_data['bbox_stats']
                print(f"  Bbox areas: {bbox_stats['areas']['min']:.0f}-{bbox_stats['areas']['max']:.0f} (avg: {bbox_stats['areas']['mean']:.0f})")
                print(f"  Bbox aspect ratios: {bbox_stats['aspect_ratios']['min']:.2f}-{bbox_stats['aspect_ratios']['max']:.2f} (avg: {bbox_stats['aspect_ratios']['mean']:.2f})")
        
        # é—®é¢˜æŠ¥å‘Š
        if self.issues:
            print(f"\nâŒ Issues Found ({len(self.issues)}):")
            for issue in self.issues:
                print(f"  - {issue}")
        else:
            print(f"\nâœ… No issues found! Dataset is valid.")
        
        print("="*60)
    
    def visualize_statistics(self, results: Dict, output_dir: str = None):
        """å¯è§†åŒ–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if output_dir is None:
            output_dir = Path(__file__).parent.parent.parent / "outputs" / "data_analysis"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Dataset Statistics', fontsize=16)
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_bbox_areas = []
        all_aspect_ratios = []
        split_labels = []
        
        for split in ['train', 'val']:
            if split in results and 'error' not in results[split]:
                # è¿™é‡Œéœ€è¦é‡æ–°åŠ è½½æ•°æ®æ¥è·å–è¯¦ç»†ç»Ÿè®¡
                pass
        
        # æš‚æ—¶æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        axes[0, 0].text(0.5, 0.5, f"Total Images: {results['summary']['total_images']}\n"
                                   f"Total Annotations: {results['summary']['total_annotations']}\n"
                                   f"Avg Ann/Image: {results['summary']['avg_annotations_per_image']:.2f}",
                        ha='center', va='center', fontsize=14, transform=axes[0, 0].transAxes)
        axes[0, 0].set_title('Dataset Overview')
        axes[0, 0].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'dataset_statistics.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Statistics visualization saved to: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•°æ®é›†éªŒè¯è„šæœ¬')
    parser.add_argument('--data-root', type=str, default='data/detection',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--visualize', action='store_true',
                       help='ç”Ÿæˆç»Ÿè®¡å¯è§†åŒ–')
    parser.add_argument('--output-dir', type=str,
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–éªŒè¯å™¨
    validator = DatasetValidator(args.data_root)
    
    # è¿è¡ŒéªŒè¯
    results = validator.validate_detection_dataset()
    
    # ç”Ÿæˆå¯è§†åŒ–
    if args.visualize:
        validator.visualize_statistics(results, args.output_dir)
    
    # è¿”å›é€‚å½“çš„é€€å‡ºä»£ç 
    if results['summary']['issues_found'] > 0:
        print("\nâš ï¸  Dataset validation completed with issues.")
        return 1
    else:
        print("\nâœ… Dataset validation completed successfully.")
        return 0


if __name__ == "__main__":
    exit(main()) 
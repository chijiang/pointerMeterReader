#!/usr/bin/env python3
"""
æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹ YOLO v10 æ¨¡å‹è®­ç»ƒè„šæœ¬

æ­¤è„šæœ¬å‚è€ƒæŒ‡é’ˆè¡¨è®­ç»ƒè„šæœ¬æ¶æ„ï¼Œæä¾›å®Œæ•´çš„æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹æ¨¡å‹è®­ç»ƒåŠŸèƒ½ã€‚
åŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€å¯è§†åŒ–ç­‰å®Œæ•´åŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
1. æ™ºèƒ½æ•°æ®é›†éªŒè¯å’Œé¢„å¤„ç†
2. YOLO v10æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–
3. å®Œæ•´çš„è¯„ä¼°å’Œå¯è§†åŒ–
4. æ¨¡å‹å¯¼å‡ºå’Œéƒ¨ç½²æ”¯æŒ
5. è¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Šç”Ÿæˆ

ä½œè€…: chijiang
æ—¥æœŸ: 2025-06-09
"""

import os
import sys
import yaml
import json
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import shutil
import random
import argparse
from typing import Dict, List, Tuple, Optional, Union
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥ultralyticsåº“
try:
    from ultralytics import YOLO
    from ultralytics.utils import LOGGER
    from ultralytics.utils.plotting import Annotator, colors
except ImportError:
    print("âŒ é”™è¯¯ï¼šè¯·å®‰è£…ultralyticsåº“")
    print("è¿è¡Œ: pip install ultralytics")
    sys.exit(1)

class DigitalMeterDetectionTrainer:
    """æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str = "config/digital_meter_yolo_config.yaml"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆæ™ºèƒ½è·¯å¾„æ£€æµ‹ï¼‰
        self.project_root = self._get_project_root()
        
        # ç¡®ä¿é…ç½®æ–‡ä»¶è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        if not os.path.isabs(config_path):
            self.config_path = self.project_root / config_path
        else:
            self.config_path = Path(config_path)
            
        self.config = self.load_config()
        self.setup_directories()
        
        # è®¾ç½®éšæœºç§å­
        self.set_seed(self.config.get('seed', 42))
        
        print(f"ğŸš€ æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹è®­ç»ƒå™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        print(f"ğŸ“Š è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _get_project_root(self) -> Path:
        """æ™ºèƒ½è·å–é¡¹ç›®æ ¹ç›®å½•"""
        current_dir = Path.cwd()
        if current_dir.name == "training":
            return current_dir.parent.parent.parent
        elif current_dir.name == "digital_meter_detection":
            return current_dir.parent.parent
        elif current_dir.name == "scripts":
            return current_dir.parent
        else:
            return current_dir
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def setup_directories(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_name = self.config.get('experiment_name', 'digital_meter_detection')
        
        # è¾“å‡ºç›®å½•
        self.output_dir = self.project_root / "outputs" / "digital_meter_detection"
        self.checkpoint_dir = self.output_dir / "checkpoints" / f"{self.experiment_name}_{self.timestamp}"
        self.log_dir = self.output_dir / "logs" / f"{self.experiment_name}_{self.timestamp}"
        self.result_dir = self.output_dir / "results" / f"{self.experiment_name}_{self.timestamp}"
        self.viz_dir = self.result_dir / "visualizations"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.checkpoint_dir, self.log_dir, self.result_dir, self.viz_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ:")
        print(f"  - æ£€æŸ¥ç‚¹: {self.checkpoint_dir}")
        print(f"  - æ—¥å¿—: {self.log_dir}")
        print(f"  - ç»“æœ: {self.result_dir}")
        print(f"  - å¯è§†åŒ–: {self.viz_dir}")
    
    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        print(f"ğŸ² éšæœºç§å­è®¾ç½®ä¸º: {seed}")
    
    def _get_device(self) -> str:
        """æ™ºèƒ½æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        config_device = self.config.get('device', 'auto')
        
        if config_device != 'auto':
            return config_device
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if torch.cuda.is_available():
            device = '0'
            print(f"ğŸ”¥ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name(0)}")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸ ä½¿ç”¨Apple MPSåŠ é€Ÿ")
        else:
            device = 'cpu'
            print("ğŸ’» ä½¿ç”¨CPU")
        
        print(f"ğŸ¯ è®¾å¤‡é€‰æ‹©: {device}")
        return device
    
    def validate_dataset(self) -> bool:
        """éªŒè¯æ•°æ®é›†æ ¼å¼å’Œå®Œæ•´æ€§"""
        print("\nğŸ” éªŒè¯æ•°æ®é›†...")
        
        dataset_path = Path(self.config['dataset']['path'])
        if not dataset_path.is_absolute():
            dataset_path = self.project_root / dataset_path
        
        # æ£€æŸ¥åŸºæœ¬ç›®å½•ç»“æ„
        images_dir = dataset_path / "images"
        labels_dir = dataset_path / "labels"
        
        if not images_dir.exists():
            print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {images_dir}")
            return False
            
        if not labels_dir.exists():
            print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {labels_dir}")
            return False
        
        # æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶
        image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.jpeg")) + list(images_dir.glob("*.png"))
        label_files = list(labels_dir.glob("*.txt"))
        
        print(f"ğŸ“¸ æ‰¾åˆ°å›¾åƒæ–‡ä»¶: {len(image_files)} ä¸ª")
        print(f"ğŸ·ï¸  æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {len(label_files)} ä¸ª")
        
        if len(image_files) == 0:
            print("âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return False
        
        # æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶çš„å¯¹åº”å…³ç³»
        missing_labels = []
        invalid_labels = []
        total_annotations = 0
        
        for img_file in tqdm(image_files, desc="éªŒè¯æ–‡ä»¶å¯¹åº”å…³ç³»"):
            label_file = labels_dir / f"{img_file.stem}.txt"
            
            if not label_file.exists():
                missing_labels.append(img_file.name)
                continue
            
            # éªŒè¯æ ‡ç­¾æ ¼å¼
            try:
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                
                for line_num, line in enumerate(lines, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    parts = line.split()
                    if len(parts) != 5:
                        invalid_labels.append(f"{label_file.name}:{line_num}")
                        continue
                    
                    # æ£€æŸ¥ç±»åˆ«IDå’Œåæ ‡
                    try:
                        class_id = int(parts[0])
                        coords = [float(x) for x in parts[1:]]
                        
                        if class_id != 0:  # æ¶²æ™¶è¡¨åªæœ‰ä¸€ä¸ªç±»åˆ«
                            invalid_labels.append(f"{label_file.name}:{line_num} - é”™è¯¯ç±»åˆ«ID: {class_id}")
                            continue
                        
                        for coord in coords:
                            if not (0 <= coord <= 1):
                                invalid_labels.append(f"{label_file.name}:{line_num} - åæ ‡è¶…å‡ºèŒƒå›´")
                                break
                        else:
                            total_annotations += 1
                            
                    except ValueError:
                        invalid_labels.append(f"{label_file.name}:{line_num} - æ ¼å¼é”™è¯¯")
                        
            except Exception as e:
                invalid_labels.append(f"{label_file.name} - è¯»å–é”™è¯¯: {e}")
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        print(f"\nğŸ“Š æ•°æ®é›†éªŒè¯ç»“æœ:")
        print(f"  âœ… æœ‰æ•ˆå›¾åƒ: {len(image_files) - len(missing_labels)} ä¸ª")
        print(f"  âœ… æœ‰æ•ˆæ ‡æ³¨: {total_annotations} ä¸ª")
        
        if missing_labels:
            print(f"  âš ï¸  ç¼ºå°‘æ ‡ç­¾: {len(missing_labels)} ä¸ª")
        
        if invalid_labels:
            print(f"  âŒ æ— æ•ˆæ ‡ç­¾: {len(invalid_labels)} ä¸ª")
            for error in invalid_labels[:5]:
                print(f"    - {error}")
            if len(invalid_labels) > 5:
                print(f"    ... è¿˜æœ‰ {len(invalid_labels) - 5} ä¸ªé”™è¯¯")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        validation_report = {
            'timestamp': datetime.now().isoformat(),
            'total_images': len(image_files),
            'total_labels': len(label_files),
            'valid_annotations': total_annotations,
            'missing_labels': len(missing_labels),
            'invalid_labels': len(invalid_labels),
            'validation_passed': len(invalid_labels) == 0
        }
        
        report_file = self.result_dir / "dataset_validation_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(validation_report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return len(invalid_labels) == 0
    
    def prepare_yolo_dataset(self) -> str:
        """å‡†å¤‡YOLOæ ¼å¼æ•°æ®é›†é…ç½®"""
        print("\nğŸ“¦ å‡†å¤‡YOLOæ•°æ®é›†é…ç½®...")
        
        dataset_path = Path(self.config['dataset']['path'])
        if not dataset_path.is_absolute():
            dataset_path = self.project_root / dataset_path
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
        dataset_config = {
            'path': str(dataset_path),
            'train': 'images',
            'val': 'images',  # ä½¿ç”¨ç›¸åŒç›®å½•ï¼Œé€šè¿‡splitå‚æ•°åˆ†å‰²
            'nc': 1,
            'names': ['digital_meter']
        }
        
        config_file = dataset_path / "dataset.yaml"
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… YOLOæ•°æ®é›†é…ç½®åˆ›å»º: {config_file}")
        return str(config_file)
    
    def visualize_dataset(self, num_samples: int = 16):
        """å¯è§†åŒ–æ•°æ®é›†æ ·æœ¬"""
        print(f"\nğŸ¨ å¯è§†åŒ–æ•°æ®é›†æ ·æœ¬ ({num_samples} ä¸ª)...")
        
        dataset_path = Path(self.config['dataset']['path'])
        if not dataset_path.is_absolute():
            dataset_path = self.project_root / dataset_path
        
        images_dir = dataset_path / "images"
        labels_dir = dataset_path / "labels"
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.jpeg")) + list(images_dir.glob("*.png"))
        selected_files = random.sample(image_files, min(num_samples, len(image_files)))
        
        # è®¾ç½®ç»˜å›¾
        fig, axes = plt.subplots(4, 4, figsize=(16, 16))
        fig.suptitle('æ¶²æ™¶æ•°å­—è¡¨æ•°æ®é›†æ ·æœ¬', fontsize=16, fontweight='bold')
        
        for idx, img_file in enumerate(selected_files):
            if idx >= 16:
                break
                
            row, col = idx // 4, idx % 4
            ax = axes[row, col]
            
            # åŠ è½½å›¾åƒ
            image = cv2.imread(str(img_file))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            height, width = image.shape[:2]
            
            # åŠ è½½æ ‡ç­¾
            label_file = labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                        
                    parts = line.split()
                    if len(parts) == 5:
                        class_id, x_center, y_center, w, h = map(float, parts)
                        
                        # è½¬æ¢ä¸ºåƒç´ åæ ‡
                        x1 = int((x_center - w/2) * width)
                        y1 = int((y_center - h/2) * height)
                        x2 = int((x_center + w/2) * width)
                        y2 = int((y_center + h/2) * height)
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)
                        cv2.putText(image, 'digital_meter', (x1, y1-10), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
            
            ax.imshow(image)
            ax.set_title(f'æ ·æœ¬ {idx+1}: {img_file.name}', fontsize=10)
            ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(selected_files), 16):
            row, col = idx // 4, idx % 4
            axes[row, col].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        viz_file = self.viz_dir / "dataset_samples.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ•°æ®é›†å¯è§†åŒ–ä¿å­˜: {viz_file}")
    
    def train(self) -> str:
        """è®­ç»ƒYOLO v10æ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹YOLO v10æ¨¡å‹è®­ç»ƒ...")
        
        # éªŒè¯æ•°æ®é›†
        if not self.validate_dataset():
            print("âŒ æ•°æ®é›†éªŒè¯å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
            return None
        
        # å¯è§†åŒ–æ•°æ®é›†
        self.visualize_dataset()
        
        # å‡†å¤‡æ•°æ®é›†é…ç½®
        dataset_config_path = self.prepare_yolo_dataset()
        
        # åˆå§‹åŒ–æ¨¡å‹
        model_name = self.config.get('model', 'yolov10n.pt')
        print(f"ğŸ¤– åˆå§‹åŒ–æ¨¡å‹: {model_name}")
        model = YOLO(model_name)
        
        # è®­ç»ƒå‚æ•°
        train_args = self._get_training_args(dataset_config_path)
        
        print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        for key, value in train_args.items():
            print(f"  {key}: {value}")
        
        # å¼€å§‹è®­ç»ƒ
        try:
            results = model.train(**train_args)
            
            # è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
            best_model_path = Path(results.save_dir) / "weights" / "best.pt"
            
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            print(f"ğŸ“ æœ€ä½³æ¨¡å‹: {best_model_path}")
            
            # å¤åˆ¶æ¨¡å‹åˆ°ç»“æœç›®å½•
            shutil.copy2(best_model_path, self.result_dir / "best_model.pt")
            
            return str(best_model_path)
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            raise
    
    def _get_training_args(self, dataset_config_path: str) -> Dict:
        """è·å–è®­ç»ƒå‚æ•°"""
        return {
            'data': dataset_config_path,
            'epochs': self.config.get('epochs', 200),
            'imgsz': self.config.get('image_size', 640),
            'batch': self.config.get('batch_size', 16),
            'lr0': self.config.get('learning_rate', 0.01),
            'weight_decay': self.config.get('weight_decay', 0.0005),
            'momentum': self.config.get('momentum', 0.937),
            'patience': self.config.get('patience', 50),
            'save_period': self.config.get('save_period', 10),
            'project': str(self.checkpoint_dir.parent),
            'name': self.checkpoint_dir.name,
            'exist_ok': True,
            'pretrained': True,
            'optimize': True,
            'verbose': True,
            'seed': self.config.get('seed', 42),
            'deterministic': True,
            'single_cls': True,
            'device': self._get_device(),
            'workers': self.config.get('workers', 8),
            'cos_lr': self.config.get('cos_lr', True),
            'close_mosaic': self.config.get('close_mosaic', 10),
            'amp': self.config.get('amp', True),
            'split': self.config.get('train_split', 0.8),
            **self._get_augmentation_args()
        }
    
    def _get_augmentation_args(self) -> Dict:
        """è·å–æ•°æ®å¢å¼ºå‚æ•°"""
        augment = self.config.get('augmentation', {})
        return {
            'hsv_h': augment.get('hsv_h', 0.01),
            'hsv_s': augment.get('hsv_s', 0.6),
            'hsv_v': augment.get('hsv_v', 0.5),
            'degrees': augment.get('degrees', 10.0),
            'translate': augment.get('translate', 0.2),
            'scale': augment.get('scale', 0.6),
            'shear': augment.get('shear', 2.0),
            'perspective': augment.get('perspective', 0.0002),
            'flipud': augment.get('flipud', 0.0),
            'fliplr': augment.get('fliplr', 0.5),
            'mosaic': augment.get('mosaic', 1.0),
            'mixup': augment.get('mixup', 0.1),
            'copy_paste': augment.get('copy_paste', 0.1),
        }
    
    def evaluate(self, model_path: str) -> Dict:
        """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åŠ è½½æ¨¡å‹
        model = YOLO(model_path)
        
        # å‡†å¤‡éªŒè¯æ•°æ®
        dataset_config_path = self.prepare_yolo_dataset()
        
        # è¿è¡ŒéªŒè¯
        results = model.val(
            data=dataset_config_path,
            imgsz=self.config.get('image_size', 640),
            batch=self.config.get('batch_size', 16),
            conf=0.001,
            iou=0.6,
            max_det=300,
            device=self._get_device(),
            save_json=True,
            save_hybrid=True,
                plots=True,
            verbose=True,
            split='val'
        )
        
        # æå–è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'mAP50': float(results.box.map50) if hasattr(results.box, 'map50') else 0.0,
            'mAP50-95': float(results.box.map) if hasattr(results.box, 'map') else 0.0,
            'precision': float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,
            'recall': float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,
        }
        
        # è®¡ç®—F1åˆ†æ•°
        if metrics['precision'] + metrics['recall'] > 0:
            metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])
        else:
            metrics['f1_score'] = 0.0
        
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        results_file = self.result_dir / "evaluation_metrics.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        return metrics
    
    def visualize_predictions(self, model_path: str, num_samples: int = 16):
        """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹ç»“æœ"""
        print(f"\nğŸ¨ å¯è§†åŒ–é¢„æµ‹ç»“æœ ({num_samples} ä¸ªæ ·æœ¬)...")
        
        # åŠ è½½æ¨¡å‹
        model = YOLO(model_path)
        
        # è·å–æµ‹è¯•å›¾åƒ
        dataset_path = Path(self.config['dataset']['path'])
        if not dataset_path.is_absolute():
            dataset_path = self.project_root / dataset_path
        
        images_dir = dataset_path / "images"
        image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.jpeg")) + list(images_dir.glob("*.png"))
        selected_files = random.sample(image_files, min(num_samples, len(image_files)))
        
        # è®¾ç½®ç»˜å›¾
        fig, axes = plt.subplots(4, 4, figsize=(20, 20))
        fig.suptitle('æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹é¢„æµ‹ç»“æœ', fontsize=16, fontweight='bold')
        
        for idx, img_file in enumerate(selected_files):
            if idx >= 16:
                break
                
            row, col = idx // 4, idx % 4
            ax = axes[row, col]
            
            # é¢„æµ‹
            results = model(str(img_file), conf=0.25, verbose=False)
            
            # ç»˜åˆ¶ç»“æœ
            annotated_img = results[0].plot(conf=True, line_width=2, font_size=12)
            annotated_img = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)
            
            ax.imshow(annotated_img)
            ax.set_title(f'é¢„æµ‹ {idx+1}: {img_file.name}', fontsize=10)
            ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(selected_files), 16):
            row, col = idx // 4, idx % 4
            axes[row, col].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        viz_file = self.viz_dir / "prediction_results.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… é¢„æµ‹å¯è§†åŒ–ä¿å­˜: {viz_file}")
    
    def plot_training_curves(self, model_path: str):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        
        # æŸ¥æ‰¾è®­ç»ƒç»“æœç›®å½•
        model_dir = Path(model_path).parent.parent
        results_csv = model_dir / "results.csv"
        
        if not results_csv.exists():
            print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒç»“æœæ–‡ä»¶: {results_csv}")
            return
        
        # è¯»å–è®­ç»ƒç»“æœ
        import pandas as pd
        df = pd.read_csv(results_csv)
        df.columns = df.columns.str.strip()  # æ¸…ç†åˆ—å
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('è®­ç»ƒæ›²çº¿åˆ†æ', fontsize=16, fontweight='bold')
        
        # æŸå¤±æ›²çº¿
        if 'train/box_loss' in df.columns and 'val/box_loss' in df.columns:
            axes[0, 0].plot(df['epoch'], df['train/box_loss'], label='è®­ç»ƒæŸå¤±', color='blue')
            axes[0, 0].plot(df['epoch'], df['val/box_loss'], label='éªŒè¯æŸå¤±', color='red')
            axes[0, 0].set_title('è¾¹ç•Œæ¡†æŸå¤±')
            axes[0, 0].set_xlabel('è½®æ¬¡')
            axes[0, 0].set_ylabel('æŸå¤±å€¼')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # ç²¾åº¦æ›²çº¿
        if 'metrics/precision(B)' in df.columns:
            axes[0, 1].plot(df['epoch'], df['metrics/precision(B)'], label='ç²¾åº¦', color='green')
            axes[0, 1].set_title('ç²¾åº¦æ›²çº¿')
            axes[0, 1].set_xlabel('è½®æ¬¡')
            axes[0, 1].set_ylabel('ç²¾åº¦')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # å¬å›ç‡æ›²çº¿
        if 'metrics/recall(B)' in df.columns:
            axes[0, 2].plot(df['epoch'], df['metrics/recall(B)'], label='å¬å›ç‡', color='orange')
            axes[0, 2].set_title('å¬å›ç‡æ›²çº¿')
            axes[0, 2].set_xlabel('è½®æ¬¡')
            axes[0, 2].set_ylabel('å¬å›ç‡')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # mAPæ›²çº¿
        if 'metrics/mAP50(B)' in df.columns:
            axes[1, 0].plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', color='purple')
            axes[1, 0].set_title('mAP@0.5æ›²çº¿')
            axes[1, 0].set_xlabel('è½®æ¬¡')
            axes[1, 0].set_ylabel('mAP@0.5')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        if 'metrics/mAP50-95(B)' in df.columns:
            axes[1, 1].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', color='brown')
            axes[1, 1].set_title('mAP@0.5:0.95æ›²çº¿')
            axes[1, 1].set_xlabel('è½®æ¬¡')
            axes[1, 1].set_ylabel('mAP@0.5:0.95')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if 'lr/pg0' in df.columns:
            axes[1, 2].plot(df['epoch'], df['lr/pg0'], label='å­¦ä¹ ç‡', color='red')
            axes[1, 2].set_title('å­¦ä¹ ç‡æ›²çº¿')
            axes[1, 2].set_xlabel('è½®æ¬¡')
            axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜è®­ç»ƒæ›²çº¿
        curves_file = self.viz_dir / "training_curves.png"
        plt.savefig(curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è®­ç»ƒæ›²çº¿ä¿å­˜: {curves_file}")
    
    def export_model(self, model_path: str, formats: List[str] = ['onnx']):
        """å¯¼å‡ºæ¨¡å‹åˆ°ä¸åŒæ ¼å¼"""
        print(f"\nğŸ“¦ å¯¼å‡ºæ¨¡å‹...")
        
        model = YOLO(model_path)
        
        export_dir = self.result_dir / "exported_models"
        export_dir.mkdir(exist_ok=True)
        
        for format_name in formats:
            try:
                print(f"  å¯¼å‡ºåˆ° {format_name.upper()}...")
                exported_model = model.export(
                    format=format_name,
                    imgsz=self.config.get('image_size', 640),
                    optimize=True,
                    half=False,
                    int8=False,
                    dynamic=False,
                    simplify=True,
                    opset=None,
                    workspace=4,
                    nms=False
                )
                
                # å¤åˆ¶åˆ°ç»“æœç›®å½•
                if exported_model and Path(exported_model).exists():
                    export_file = export_dir / Path(exported_model).name
                    shutil.copy2(exported_model, export_file)
                    print(f"  âœ… {format_name.upper()} æ¨¡å‹å¯¼å‡º: {export_file}")
                
            except Exception as e:
                    print(f"  âŒ {format_name} å¯¼å‡ºå¤±è´¥: {str(e)}")
    
    def generate_report(self, model_path: str, metrics: Dict):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        report = {
            'experiment_info': {
                'name': self.experiment_name,
                'timestamp': self.timestamp,
                'model_path': str(model_path),
                'config_path': str(self.config_path)
            },
            'dataset_info': {
                'path': self.config['dataset']['path'],
                'classes': ['digital_meter'],
                'num_classes': 1
            },
            'training_config': {
                'model': self.config.get('model', 'yolov10n.pt'),
                'epochs': self.config.get('epochs', 200),
                'batch_size': self.config.get('batch_size', 16),
                'image_size': self.config.get('image_size', 640),
                'device': self._get_device()
            },
            'performance_metrics': metrics,
            'files': {
                'best_model': str(self.result_dir / "best_model.pt"),
                'training_curves': str(self.viz_dir / "training_curves.png"),
                'predictions': str(self.viz_dir / "prediction_results.png"),
                'dataset_samples': str(self.viz_dir / "dataset_samples.png")
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = self.result_dir / "training_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
        
        print(f"âœ… è®­ç»ƒæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file}")
    
    def _generate_markdown_report(self, report: Dict):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹è®­ç»ƒæŠ¥å‘Š

## å®éªŒä¿¡æ¯
- **å®éªŒåç§°**: {report['experiment_info']['name']}
- **æ—¶é—´æˆ³**: {report['experiment_info']['timestamp']}
- **æ¨¡å‹è·¯å¾„**: {report['experiment_info']['model_path']}

## æ•°æ®é›†ä¿¡æ¯
- **æ•°æ®é›†è·¯å¾„**: {report['dataset_info']['path']}
- **ç±»åˆ«æ•°é‡**: {report['dataset_info']['num_classes']}
- **ç±»åˆ«**: {', '.join(report['dataset_info']['classes'])}

## è®­ç»ƒé…ç½®
- **åŸºç¡€æ¨¡å‹**: {report['training_config']['model']}
- **è®­ç»ƒè½®æ•°**: {report['training_config']['epochs']}
- **æ‰¹æ¬¡å¤§å°**: {report['training_config']['batch_size']}
- **å›¾åƒå°ºå¯¸**: {report['training_config']['image_size']}
- **è®¾å¤‡**: {report['training_config']['device']}

## æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| mAP@0.5 | {report['performance_metrics']['mAP50']:.4f} |
| mAP@0.5:0.95 | {report['performance_metrics']['mAP50-95']:.4f} |
| ç²¾åº¦ | {report['performance_metrics']['precision']:.4f} |
| å¬å›ç‡ | {report['performance_metrics']['recall']:.4f} |
| F1åˆ†æ•° | {report['performance_metrics']['f1_score']:.4f} |

## ç”Ÿæˆæ–‡ä»¶
- æœ€ä½³æ¨¡å‹: `best_model.pt`
- è®­ç»ƒæ›²çº¿: `visualizations/training_curves.png`
- é¢„æµ‹ç»“æœ: `visualizations/prediction_results.png`
- æ•°æ®é›†æ ·æœ¬: `visualizations/dataset_samples.png`

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        
        md_file = self.result_dir / "training_report.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def run_complete_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. è®­ç»ƒæ¨¡å‹
            best_model_path = self.train()
            if not best_model_path:
                return
            
            # 2. è¯„ä¼°æ¨¡å‹
            metrics = self.evaluate(best_model_path)
            
            # 3. å¯è§†åŒ–é¢„æµ‹ç»“æœ
            self.visualize_predictions(best_model_path)
            
            # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            self.plot_training_curves(best_model_path)
            
            # 5. å¯¼å‡ºæ¨¡å‹
            self.export_model(best_model_path, ['onnx', 'torchscript'])
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            self.generate_report(best_model_path, metrics)
            
            print(f"\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
            print(f"ğŸ“ ç»“æœç›®å½•: {self.result_dir}")
            print(f"ğŸ“Š æœ€ç»ˆmAP@0.5: {metrics['mAP50']:.4f}")
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæµç¨‹å¤±è´¥: {str(e)}")
            raise


def create_default_config() -> Dict:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'experiment_name': 'digital_meter_detection',
        'model': 'yolov10n.pt',
        'dataset': {
            'path': 'data/digital_meters'
        },
        'epochs': 200,
        'batch_size': 16,
        'image_size': 640,
        'learning_rate': 0.01,
        'weight_decay': 0.0005,
        'momentum': 0.937,
        'patience': 50,
        'save_period': 10,
        'device': 'auto',
        'workers': 8,
        'seed': 42,
        'cos_lr': True,
        'close_mosaic': 10,
        'amp': True,
        'train_split': 0.8,
        'augmentation': {
            'hsv_h': 0.01,
            'hsv_s': 0.6,
            'hsv_v': 0.5,
            'degrees': 10.0,
            'translate': 0.2,
            'scale': 0.6,
            'shear': 2.0,
            'perspective': 0.0002,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 1.0,
            'mixup': 0.1,
            'copy_paste': 0.1
        }
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹YOLO v10è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config/digital_meter_yolo_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰')
    parser.add_argument('--create-config', action='store_true',
                       help='åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶')
    parser.add_argument('--validate-only', action='store_true',
                       help='ä»…éªŒè¯æ•°æ®é›†')
    parser.add_argument('--eval-only', action='store_true',
                       help='ä»…è¯„ä¼°æ¨¡å‹')
    parser.add_argument('--model-path', type=str,
                       help='æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºè¯„ä¼°ï¼‰')
    parser.add_argument('--export', action='store_true',
                       help='å¯¼å‡ºæ¨¡å‹')
    parser.add_argument('--formats', nargs='+', default=['onnx'],
                       help='å¯¼å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé»˜è®¤é…ç½®
    if args.create_config:
        config_path = Path(args.config)
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        default_config = create_default_config()
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… é»˜è®¤é…ç½®æ–‡ä»¶åˆ›å»º: {config_path}")
        return
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("ğŸ’¡ ä½¿ç”¨ --create-config åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")
        return
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = DigitalMeterDetectionTrainer(args.config)
    
    try:
        if args.validate_only:
            # ä»…éªŒè¯æ•°æ®é›†
                trainer.validate_dataset()
                trainer.visualize_dataset()
                
        elif args.eval_only:
            # ä»…è¯„ä¼°æ¨¡å‹
            if not args.model_path:
                print("âŒ è¯„ä¼°æ¨¡å¼éœ€è¦æä¾›æ¨¡å‹è·¯å¾„")
                return
            
            metrics = trainer.evaluate(args.model_path)
            trainer.visualize_predictions(args.model_path)
            trainer.plot_training_curves(args.model_path)
            
            if args.export:
                trainer.export_model(args.model_path, args.formats)
        
            else:
        # è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
                trainer.run_complete_training()
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹æ¨ç†è„šæœ¬

æ­¤è„šæœ¬å‚è€ƒæŒ‡é’ˆè¡¨è®­ç»ƒè„šæœ¬æ¶æ„ï¼Œæä¾›å®Œæ•´çš„æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹æ¨ç†åŠŸèƒ½ã€‚
åŒ…å«æ£€æµ‹ã€è¿‡æ»¤ã€å¯è§†åŒ–ã€åˆ†æç­‰å®Œæ•´åŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
1. æ™ºèƒ½æ¨¡å‹åŠ è½½å’Œè®¾å¤‡é€‰æ‹©
2. å•å¼ å›¾åƒå’Œæ‰¹é‡æ¨ç†
3. æ™ºèƒ½ç»“æœè¿‡æ»¤å’Œåå¤„ç†
4. ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½
5. è¯¦ç»†çš„åˆ†ææŠ¥å‘Šç”Ÿæˆ
6. ROIæå–å’Œä¿å­˜

ä½œè€…: chijiang
æ—¥æœŸ: 2025-06-09
"""

import os
import sys
import cv2
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore")

# å¯¼å…¥ultralyticsåº“
try:
    from ultralytics import YOLO
    from ultralytics.utils.plotting import Annotator, colors
except ImportError:
    print("âŒ é”™è¯¯ï¼šè¯·å®‰è£…ultralyticsåº“")
    print("è¿è¡Œ: pip install ultralytics")
    sys.exit(1)


class DigitalMeterDetector:
    """æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹å™¨"""

    def __init__(
        self,
        model_path: str,
        conf_threshold: float = 0.25,
        iou_threshold: float = 0.5,
        device: str = "auto",
    ):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨

        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IoUé˜ˆå€¼
            device: è®¾å¤‡ç±»å‹
        """
        self.model_path = Path(model_path)
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.device = self._get_device(device)

        # è·å–é¡¹ç›®æ ¹ç›®å½•
        self.project_root = self._get_project_root()

        # è§£ææ¨¡å‹è·¯å¾„
        if not self.model_path.is_absolute():
            self.model_path = self.project_root / self.model_path

        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()

        # è®¾ç½®è¾“å‡ºç›®å½•
        self.setup_output_dirs()

        # è¿‡æ»¤å‚æ•°
        self.filter_config = {
            "min_area": 1600,  # æœ€å°é¢ç§¯ï¼ˆåƒç´ ï¼‰
            "max_area": 360000,  # æœ€å¤§é¢ç§¯ï¼ˆåƒç´ ï¼‰
            "min_aspect_ratio": 1.2,  # æœ€å°å®½é«˜æ¯”
            "max_aspect_ratio": 6.0,  # æœ€å¤§å®½é«˜æ¯”
        }

        print(f"ğŸš€ æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹å™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"ğŸ¯ è®¾å¤‡: {self.device}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold}")

    def _get_project_root(self) -> Path:
        """æ™ºèƒ½è·å–é¡¹ç›®æ ¹ç›®å½•"""
        current_dir = Path.cwd()
        if current_dir.name == "inference":
            return current_dir.parent.parent.parent
        elif current_dir.name == "digital_meter_detection":
            return current_dir.parent.parent
        elif current_dir.name == "scripts":
            return current_dir.parent
        else:
            return current_dir

    def _get_device(self, device: str) -> str:
        """æ™ºèƒ½æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        if device != "auto":
            return device

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        import torch

        if torch.cuda.is_available():
            device = "0"
            print(f"ğŸ”¥ ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name(0)}")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = "mps"
            print("ğŸ ä½¿ç”¨Apple MPSåŠ é€Ÿ")
        else:
            device = "cpu"
            print("ğŸ’» ä½¿ç”¨CPU")

        return device

    def _load_model(self) -> YOLO:
        """åŠ è½½YOLOæ¨¡å‹"""
        try:
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")

            if not self.model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")

            model = YOLO(str(self.model_path))

            # è®¾ç½®è®¾å¤‡
            model.to(self.device)

            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

    def setup_output_dirs(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # è¾“å‡ºç›®å½•ç»“æ„
        self.output_root = self.project_root / "outputs" / "digital_meter_inference"
        self.result_dir = self.output_root / f"inference_{self.timestamp}"
        self.viz_dir = self.result_dir / "visualizations"
        self.roi_dir = self.result_dir / "rois"
        self.analysis_dir = self.result_dir / "analysis"

        # åˆ›å»ºç›®å½•
        for dir_path in [
            self.result_dir,
            self.viz_dir,
            self.roi_dir,
            self.analysis_dir,
        ]:
            dir_path.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"  - ç»“æœ: {self.result_dir}")
        print(f"  - å¯è§†åŒ–: {self.viz_dir}")
        print(f"  - ROI: {self.roi_dir}")
        print(f"  - åˆ†æ: {self.analysis_dir}")

    def detect_single_image(self, image_path: Union[str, Path]) -> Dict:
        """
        æ£€æµ‹å•å¼ å›¾åƒ

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        image_path = Path(image_path)

        if not image_path.exists():
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

        print(f"ğŸ” æ£€æµ‹å›¾åƒ: {image_path.name}")

        # æ‰§è¡Œæ£€æµ‹
        results = self.model(
            str(image_path),
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            device=self.device,
            verbose=False,
        )

        # è§£æç»“æœ
        detections = self._parse_detections(results[0], image_path)

        # è¿‡æ»¤ç»“æœ
        filtered_detections = self._filter_detections(detections)

        return {
            "image_path": str(image_path),
            "image_name": image_path.name,
            "raw_detections": detections,
            "filtered_detections": filtered_detections,
            "detection_count": len(filtered_detections),
            "timestamp": datetime.now().isoformat(),
        }

    def _parse_detections(self, result, image_path: Path) -> List[Dict]:
        """è§£ææ£€æµ‹ç»“æœ"""
        detections = []

        if result.boxes is not None:
            boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2
            confs = result.boxes.conf.cpu().numpy()
            classes = result.boxes.cls.cpu().numpy()

            # è·å–å›¾åƒå°ºå¯¸
            image = cv2.imread(str(image_path))
            img_height, img_width = image.shape[:2]

            for i, (box, conf, cls) in enumerate(zip(boxes, confs, classes)):
                x1, y1, x2, y2 = box
                width = x2 - x1
                height = y2 - y1
                area = width * height
                aspect_ratio = width / height if height > 0 else 0

                detection = {
                    "id": i,
                    "bbox": [float(x1), float(y1), float(x2), float(y2)],
                    "confidence": float(conf),
                    "class_id": int(cls),
                    "class_name": "digital_meter",
                    "width": float(width),
                    "height": float(height),
                    "area": float(area),
                    "aspect_ratio": float(aspect_ratio),
                    "center_x": float((x1 + x2) / 2),
                    "center_y": float((y1 + y2) / 2),
                    "relative_center_x": float((x1 + x2) / 2 / img_width),
                    "relative_center_y": float((y1 + y2) / 2 / img_height),
                    "relative_width": float(width / img_width),
                    "relative_height": float(height / img_height),
                }

                detections.append(detection)

        return detections

    def _filter_detections(self, detections: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤æ£€æµ‹ç»“æœ"""
        filtered = []

        for detection in detections:
            # é¢ç§¯è¿‡æ»¤
            if detection["area"] < self.filter_config["min_area"]:
                continue
            if detection["area"] > self.filter_config["max_area"]:
                continue

            # å®½é«˜æ¯”è¿‡æ»¤
            if detection["aspect_ratio"] < self.filter_config["min_aspect_ratio"]:
                continue
            if detection["aspect_ratio"] > self.filter_config["max_aspect_ratio"]:
                continue

            filtered.append(detection)

        return filtered

    def extract_roi(
        self, image_path: Union[str, Path], detection: Dict, padding: int = 20
    ) -> np.ndarray:
        """
        æå–ROIåŒºåŸŸ

        Args:
            image_path: å›¾åƒè·¯å¾„
            detection: æ£€æµ‹ç»“æœ
            padding: è¾¹ç•Œå¡«å……

        Returns:
            ROIå›¾åƒ
        """
        image = cv2.imread(str(image_path))
        height, width = image.shape[:2]

        # è·å–è¾¹ç•Œæ¡†
        x1, y1, x2, y2 = detection["bbox"]

        # æ·»åŠ å¡«å……
        x1 = max(0, int(x1 - padding))
        y1 = max(0, int(y1 - padding))
        x2 = min(width, int(x2 + padding))
        y2 = min(height, int(y2 + padding))

        # æå–ROI
        roi = image[y1:y2, x1:x2]

        return roi

    def visualize_detections(
        self, image_path: Union[str, Path], detections: List[Dict], save: bool = True
    ) -> np.ndarray:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ

        Args:
            image_path: å›¾åƒè·¯å¾„
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨
            save: æ˜¯å¦ä¿å­˜ç»“æœ

        Returns:
            å¯è§†åŒ–å›¾åƒ
        """
        # åŠ è½½å›¾åƒ
        image = cv2.imread(str(image_path))
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # åˆ›å»ºå‰¯æœ¬ç”¨äºç»˜åˆ¶
        viz_image = image_rgb.copy()

        # ç»˜åˆ¶æ£€æµ‹æ¡†
        for detection in detections:
            x1, y1, x2, y2 = [int(x) for x in detection["bbox"]]
            conf = detection["confidence"]

            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(viz_image, (x1, y1), (x2, y2), (255, 0, 0), 2)

            # ç»˜åˆ¶æ ‡ç­¾
            label = f"digital_meter {conf:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]

            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(
                viz_image,
                (x1, y1 - label_size[1] - 10),
                (x1 + label_size[0], y1),
                (255, 0, 0),
                -1,
            )

            # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—
            cv2.putText(
                viz_image,
                label,
                (x1, y1 - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 255),
                2,
            )

        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        if save:
            image_name = Path(image_path).stem
            viz_file = self.viz_dir / f"{image_name}_detections.jpg"
            viz_bgr = cv2.cvtColor(viz_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(str(viz_file), viz_bgr)
            print(f"ğŸ’¾ å¯è§†åŒ–ç»“æœä¿å­˜: {viz_file}")

        return viz_image

    def create_detection_summary(self, all_results: List[Dict]) -> Dict:
        """åˆ›å»ºæ£€æµ‹æ€»ç»“"""
        print("\nğŸ“Š ç”Ÿæˆæ£€æµ‹æ€»ç»“...")

        total_images = len(all_results)
        total_detections = sum(len(r["filtered_detections"]) for r in all_results)
        images_with_detections = sum(
            1 for r in all_results if len(r["filtered_detections"]) > 0
        )

        # ç½®ä¿¡åº¦ç»Ÿè®¡
        all_confidences = []
        all_areas = []
        all_aspect_ratios = []

        for result in all_results:
            for detection in result["filtered_detections"]:
                all_confidences.append(detection["confidence"])
                all_areas.append(detection["area"])
                all_aspect_ratios.append(detection["aspect_ratio"])

        summary = {
            "statistics": {
                "total_images": total_images,
                "images_with_detections": images_with_detections,
                "detection_rate": (
                    images_with_detections / total_images if total_images > 0 else 0
                ),
                "total_detections": total_detections,
                "avg_detections_per_image": (
                    total_detections / total_images if total_images > 0 else 0
                ),
            },
            "confidence_stats": {
                "mean": float(np.mean(all_confidences)) if all_confidences else 0,
                "std": float(np.std(all_confidences)) if all_confidences else 0,
                "min": float(np.min(all_confidences)) if all_confidences else 0,
                "max": float(np.max(all_confidences)) if all_confidences else 0,
            },
            "area_stats": {
                "mean": float(np.mean(all_areas)) if all_areas else 0,
                "std": float(np.std(all_areas)) if all_areas else 0,
                "min": float(np.min(all_areas)) if all_areas else 0,
                "max": float(np.max(all_areas)) if all_areas else 0,
            },
            "aspect_ratio_stats": {
                "mean": float(np.mean(all_aspect_ratios)) if all_aspect_ratios else 0,
                "std": float(np.std(all_aspect_ratios)) if all_aspect_ratios else 0,
                "min": float(np.min(all_aspect_ratios)) if all_aspect_ratios else 0,
                "max": float(np.max(all_aspect_ratios)) if all_aspect_ratios else 0,
            },
            "filter_config": self.filter_config,
            "model_config": {
                "model_path": str(self.model_path),
                "conf_threshold": self.conf_threshold,
                "iou_threshold": self.iou_threshold,
                "device": self.device,
            },
            "timestamp": datetime.now().isoformat(),
        }

        return summary

    def plot_detection_analysis(self, all_results: List[Dict]):
        """ç»˜åˆ¶æ£€æµ‹åˆ†æå›¾è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆåˆ†æå›¾è¡¨...")

        # æ”¶é›†æ•°æ®
        confidences = []
        areas = []
        aspect_ratios = []
        detection_counts = []

        for result in all_results:
            detection_counts.append(len(result["filtered_detections"]))
            for detection in result["filtered_detections"]:
                confidences.append(detection["confidence"])
                areas.append(detection["area"])
                aspect_ratios.append(detection["aspect_ratio"])

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle("æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹åˆ†æ", fontsize=16, fontweight="bold")

        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        if confidences:
            axes[0, 0].hist(
                confidences, bins=20, alpha=0.7, color="blue", edgecolor="black"
            )
            axes[0, 0].axvline(
                np.mean(confidences),
                color="red",
                linestyle="--",
                label=f"å‡å€¼: {np.mean(confidences):.3f}",
            )
            axes[0, 0].set_title("ç½®ä¿¡åº¦åˆ†å¸ƒ")
            axes[0, 0].set_xlabel("ç½®ä¿¡åº¦")
            axes[0, 0].set_ylabel("é¢‘æ¬¡")
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

        # é¢ç§¯åˆ†å¸ƒ
        if areas:
            axes[0, 1].hist(areas, bins=20, alpha=0.7, color="green", edgecolor="black")
            axes[0, 1].axvline(
                np.mean(areas),
                color="red",
                linestyle="--",
                label=f"å‡å€¼: {np.mean(areas):.0f}",
            )
            axes[0, 1].set_title("æ£€æµ‹æ¡†é¢ç§¯åˆ†å¸ƒ")
            axes[0, 1].set_xlabel("é¢ç§¯ (åƒç´ Â²)")
            axes[0, 1].set_ylabel("é¢‘æ¬¡")
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # å®½é«˜æ¯”åˆ†å¸ƒ
        if aspect_ratios:
            axes[0, 2].hist(
                aspect_ratios, bins=20, alpha=0.7, color="orange", edgecolor="black"
            )
            axes[0, 2].axvline(
                np.mean(aspect_ratios),
                color="red",
                linestyle="--",
                label=f"å‡å€¼: {np.mean(aspect_ratios):.2f}",
            )
            axes[0, 2].set_title("å®½é«˜æ¯”åˆ†å¸ƒ")
            axes[0, 2].set_xlabel("å®½é«˜æ¯”")
            axes[0, 2].set_ylabel("é¢‘æ¬¡")
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)

        # æ¯å›¾åƒæ£€æµ‹æ•°é‡åˆ†å¸ƒ
        axes[1, 0].hist(
            detection_counts,
            bins=max(1, max(detection_counts) if detection_counts else 1),
            alpha=0.7,
            color="purple",
            edgecolor="black",
        )
        axes[1, 0].axvline(
            np.mean(detection_counts),
            color="red",
            linestyle="--",
            label=f"å‡å€¼: {np.mean(detection_counts):.2f}",
        )
        axes[1, 0].set_title("æ¯å›¾åƒæ£€æµ‹æ•°é‡åˆ†å¸ƒ")
        axes[1, 0].set_xlabel("æ£€æµ‹æ•°é‡")
        axes[1, 0].set_ylabel("å›¾åƒæ•°é‡")
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # ç½®ä¿¡åº¦vsé¢ç§¯æ•£ç‚¹å›¾
        if confidences and areas:
            scatter = axes[1, 1].scatter(
                areas,
                confidences,
                alpha=0.6,
                c=confidences,
                cmap="viridis",
                edgecolors="black",
                linewidth=0.5,
            )
            axes[1, 1].set_title("ç½®ä¿¡åº¦ vs é¢ç§¯")
            axes[1, 1].set_xlabel("é¢ç§¯ (åƒç´ Â²)")
            axes[1, 1].set_ylabel("ç½®ä¿¡åº¦")
            axes[1, 1].grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=axes[1, 1], label="ç½®ä¿¡åº¦")

        # å®½é«˜æ¯”vsç½®ä¿¡åº¦æ•£ç‚¹å›¾
        if aspect_ratios and confidences:
            scatter2 = axes[1, 2].scatter(
                aspect_ratios,
                confidences,
                alpha=0.6,
                c=areas,
                cmap="plasma",
                edgecolors="black",
                linewidth=0.5,
            )
            axes[1, 2].set_title("å®½é«˜æ¯” vs ç½®ä¿¡åº¦")
            axes[1, 2].set_xlabel("å®½é«˜æ¯”")
            axes[1, 2].set_ylabel("ç½®ä¿¡åº¦")
            axes[1, 2].grid(True, alpha=0.3)
            if areas:
                plt.colorbar(scatter2, ax=axes[1, 2], label="é¢ç§¯")

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        analysis_file = self.analysis_dir / "detection_analysis.png"
        plt.savefig(analysis_file, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"âœ… åˆ†æå›¾è¡¨ä¿å­˜: {analysis_file}")

    def create_detection_gallery(self, all_results: List[Dict], max_images: int = 16):
        """åˆ›å»ºæ£€æµ‹ç»“æœç”»å»Š"""
        print(f"\nğŸ–¼ï¸  åˆ›å»ºæ£€æµ‹ç»“æœç”»å»Š (æœ€å¤š{max_images}å¼ )...")

        # é€‰æ‹©æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒ
        results_with_detections = [
            r for r in all_results if len(r["filtered_detections"]) > 0
        ]

        if not results_with_detections:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æ¶²æ™¶æ•°å­—è¡¨ï¼Œè·³è¿‡ç”»å»Šåˆ›å»º")
            return

        # æŒ‰æ£€æµ‹æ•°é‡æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„ç»“æœ
        results_with_detections.sort(
            key=lambda x: len(x["filtered_detections"]), reverse=True
        )
        selected_results = results_with_detections[:max_images]

        # è®¡ç®—ç½‘æ ¼å¤§å°
        n_images = len(selected_results)
        cols = min(4, n_images)
        rows = (n_images + cols - 1) // cols

        # åˆ›å»ºç”»å»Š
        fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))
        if rows == 1 and cols == 1:
            axes = [axes]
        elif rows == 1:
            axes = [axes]
        else:
            axes = axes.flatten()

        fig.suptitle("æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹ç»“æœç”»å»Š", fontsize=16, fontweight="bold")

        for idx, result in enumerate(selected_results):
            if idx >= len(axes):
                break

            ax = axes[idx]

            # åŠ è½½å’Œå¯è§†åŒ–å›¾åƒ
            image_path = result["image_path"]
            detections = result["filtered_detections"]

            # åŠ è½½å›¾åƒ
            image = cv2.imread(image_path)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # ç»˜åˆ¶æ£€æµ‹æ¡†
            for detection in detections:
                x1, y1, x2, y2 = [int(x) for x in detection["bbox"]]
                conf = detection["confidence"]

                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"{conf:.2f}"
                cv2.putText(
                    image_rgb,
                    label,
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 0, 0),
                    2,
                )

            ax.imshow(image_rgb)
            ax.set_title(
                f"{Path(image_path).name}\næ£€æµ‹æ•°: {len(detections)}", fontsize=10
            )
            ax.axis("off")

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(selected_results), len(axes)):
            axes[idx].axis("off")

        plt.tight_layout()

        # ä¿å­˜ç”»å»Š
        gallery_file = self.viz_dir / "detection_gallery.png"
        plt.savefig(gallery_file, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"âœ… æ£€æµ‹ç”»å»Šä¿å­˜: {gallery_file}")

    def process_batch(
        self, input_path: Union[str, Path], save_rois: bool = True
    ) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†å›¾åƒ

        Args:
            input_path: è¾“å…¥è·¯å¾„ï¼ˆå›¾åƒæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            save_rois: æ˜¯å¦ä¿å­˜ROIåŒºåŸŸ

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        input_path = Path(input_path)

        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        if input_path.is_file():
            image_files = [input_path]
        elif input_path.is_dir():
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.tiff"]:
                image_files.extend(input_path.glob(ext))
                image_files.extend(input_path.glob(ext.upper()))
        else:
            raise ValueError(f"è¾“å…¥è·¯å¾„æ— æ•ˆ: {input_path}")

        if not image_files:
            raise ValueError(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {input_path}")

        print(f"ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹ï¼Œå…± {len(image_files)} å¼ å›¾åƒ")

        all_results = []

        # å¤„ç†æ¯å¼ å›¾åƒ
        for image_file in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
            try:
                # æ£€æµ‹
                result = self.detect_single_image(image_file)
                all_results.append(result)

                # å¯è§†åŒ–
                self.visualize_detections(image_file, result["filtered_detections"])

                # ä¿å­˜ROI
                if save_rois and result["filtered_detections"]:
                    for i, detection in enumerate(result["filtered_detections"]):
                        roi = self.extract_roi(image_file, detection)
                        roi_filename = f"{image_file.stem}_roi_{i+1}.jpg"
                        roi_path = self.roi_dir / roi_filename
                        cv2.imwrite(str(roi_path), roi)

            except Exception as e:
                print(f"âš ï¸  å¤„ç† {image_file.name} æ—¶å‡ºé”™: {e}")
                continue

        return all_results

    def save_results(self, all_results: List[Dict], summary: Dict):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜æ£€æµ‹ç»“æœ...")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.result_dir / "detection_results.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        # ä¿å­˜æ€»ç»“
        summary_file = self.result_dir / "detection_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(summary, all_results)

        print(f"âœ… ç»“æœä¿å­˜å®Œæˆ:")
        print(f"  - è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  - æ£€æµ‹æ€»ç»“: {summary_file}")

    def _generate_markdown_report(self, summary: Dict, all_results: List[Dict]):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹æŠ¥å‘Š

## æ£€æµ‹æ€»ç»“
- **æ€»å›¾åƒæ•°**: {summary['statistics']['total_images']}
- **æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒ**: {summary['statistics']['images_with_detections']}
- **æ£€æµ‹æˆåŠŸç‡**: {summary['statistics']['detection_rate']:.2%}
- **æ€»æ£€æµ‹æ•°**: {summary['statistics']['total_detections']}
- **å¹³å‡æ¯å›¾æ£€æµ‹æ•°**: {summary['statistics']['avg_detections_per_image']:.2f}

## ç½®ä¿¡åº¦ç»Ÿè®¡
- **å‡å€¼**: {summary['confidence_stats']['mean']:.3f}
- **æ ‡å‡†å·®**: {summary['confidence_stats']['std']:.3f}
- **æœ€å°å€¼**: {summary['confidence_stats']['min']:.3f}
- **æœ€å¤§å€¼**: {summary['confidence_stats']['max']:.3f}

## æ£€æµ‹æ¡†é¢ç§¯ç»Ÿè®¡
- **å‡å€¼**: {summary['area_stats']['mean']:.0f} åƒç´ Â²
- **æ ‡å‡†å·®**: {summary['area_stats']['std']:.0f} åƒç´ Â²
- **æœ€å°å€¼**: {summary['area_stats']['min']:.0f} åƒç´ Â²
- **æœ€å¤§å€¼**: {summary['area_stats']['max']:.0f} åƒç´ Â²

## å®½é«˜æ¯”ç»Ÿè®¡
- **å‡å€¼**: {summary['aspect_ratio_stats']['mean']:.2f}
- **æ ‡å‡†å·®**: {summary['aspect_ratio_stats']['std']:.2f}
- **æœ€å°å€¼**: {summary['aspect_ratio_stats']['min']:.2f}
- **æœ€å¤§å€¼**: {summary['aspect_ratio_stats']['max']:.2f}

## æ¨¡å‹é…ç½®
- **æ¨¡å‹è·¯å¾„**: {summary['model_config']['model_path']}
- **ç½®ä¿¡åº¦é˜ˆå€¼**: {summary['model_config']['conf_threshold']}
- **IoUé˜ˆå€¼**: {summary['model_config']['iou_threshold']}
- **è®¾å¤‡**: {summary['model_config']['device']}

## è¿‡æ»¤é…ç½®
- **æœ€å°é¢ç§¯**: {summary['filter_config']['min_area']} åƒç´ Â²
- **æœ€å¤§é¢ç§¯**: {summary['filter_config']['max_area']} åƒç´ Â²
- **æœ€å°å®½é«˜æ¯”**: {summary['filter_config']['min_aspect_ratio']}
- **æœ€å¤§å®½é«˜æ¯”**: {summary['filter_config']['max_aspect_ratio']}

## ç”Ÿæˆæ–‡ä»¶
- æ£€æµ‹ç»“æœå¯è§†åŒ–: `visualizations/`
- ROIåŒºåŸŸå›¾åƒ: `rois/`
- åˆ†æå›¾è¡¨: `analysis/detection_analysis.png`
- æ£€æµ‹ç”»å»Š: `visualizations/detection_gallery.png`

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""

        md_file = self.result_dir / "detection_report.md"
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_content)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ¶²æ™¶æ•°å­—è¡¨æ£€æµ‹æ¨ç†è„šæœ¬")
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„YOLOæ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰",
    )
    parser.add_argument(
        "--input", type=str, required=True, help="è¾“å…¥å›¾åƒæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--conf", type=float, default=0.25, help="ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.25)"
    )
    parser.add_argument("--iou", type=float, default=0.5, help="IoUé˜ˆå€¼ (é»˜è®¤: 0.5)")
    parser.add_argument(
        "--device", type=str, default="auto", help="è®¾å¤‡ç±»å‹ (auto/cpu/0/mps)"
    )
    parser.add_argument("--no-rois", action="store_true", help="ä¸ä¿å­˜ROIåŒºåŸŸ")
    parser.add_argument("--output", type=str, help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    # å¤„ç†æ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    model_path = Path(args.model)
    if not model_path.is_absolute():
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        current_dir = Path.cwd()
        if current_dir.name == "inference":
            project_root = current_dir.parent.parent.parent
        elif current_dir.name == "digital_meter_detection":
            project_root = current_dir.parent.parent
        elif current_dir.name == "scripts":
            project_root = current_dir.parent
        else:
            project_root = current_dir

        model_path = project_root / model_path

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)

    args.model = str(model_path)

    try:
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = DigitalMeterDetector(
            model_path=args.model,
            conf_threshold=args.conf,
            iou_threshold=args.iou,
            device=args.device,
        )

        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰è¾“å‡ºç›®å½•
        if args.output:
            output_dir = Path(args.output)
            output_dir.mkdir(parents=True, exist_ok=True)
            detector.result_dir = output_dir
            detector.viz_dir = output_dir / "visualizations"
            detector.roi_dir = output_dir / "rois"
            detector.analysis_dir = output_dir / "analysis"

            for dir_path in [detector.viz_dir, detector.roi_dir, detector.analysis_dir]:
                dir_path.mkdir(parents=True, exist_ok=True)

        # æ‰¹é‡å¤„ç†
        results = detector.process_batch(args.input, save_rois=not args.no_rois)

        # ç”Ÿæˆæ€»ç»“
        summary = detector.create_detection_summary(results)

        # ç”Ÿæˆåˆ†æå›¾è¡¨
        detector.plot_detection_analysis(results)

        # åˆ›å»ºæ£€æµ‹ç”»å»Š
        detector.create_detection_gallery(results)

        # ä¿å­˜ç»“æœ
        detector.save_results(results, summary)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ‰ æ£€æµ‹å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†å›¾åƒ: {summary['statistics']['total_images']} å¼ ")
        print(f"ğŸ¯ æ£€æµ‹æˆåŠŸ: {summary['statistics']['images_with_detections']} å¼ ")
        print(f"ğŸ“ˆ æ£€æµ‹æˆåŠŸç‡: {summary['statistics']['detection_rate']:.2%}")
        print(f"ğŸ” æ€»æ£€æµ‹æ•°: {summary['statistics']['total_detections']} ä¸ª")
        print(f"ğŸ“ ç»“æœç›®å½•: {detector.result_dir}")

    except Exception as e:
        print(f"âŒ æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()

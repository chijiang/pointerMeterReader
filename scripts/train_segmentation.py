#!/usr/bin/env python3
"""
DeepLabV3+ åˆ†å‰²è®­ç»ƒè„šæœ¬
ç”¨äºè®­ç»ƒæŒ‡é’ˆè¡¨ç›˜çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼šèƒŒæ™¯ã€æŒ‡é’ˆã€åˆ»åº¦ä¸‰åˆ†ç±»

ä½¿ç”¨æ–¹æ³•:
python scripts/train_segmentation.py --config config/segmentation_config.yaml
"""

import os
import sys
import argparse
import yaml
import logging
from pathlib import Path
from datetime import datetime
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
import torchvision.transforms as transforms
import torchvision.models.segmentation as segmentation

from tqdm import tqdm
import cv2


class SegmentationDataset(Dataset):
    """åˆ†å‰²æ•°æ®é›†"""
    
    def __init__(self, root_dir, image_dir, mask_dir, split_file, transform=None, mask_transform=None):
        self.root_dir = Path(root_dir)
        self.image_dir = self.root_dir / image_dir
        self.mask_dir = self.root_dir / mask_dir
        self.image_transform = transform
        self.mask_transform = mask_transform
        
        # è¯»å–æ•°æ®åˆ†å‰²æ–‡ä»¶
        if split_file and os.path.exists(split_file):
            with open(split_file, 'r') as f:
                self.image_names = [line.strip() for line in f.readlines()]
        else:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ–‡ä»¶ï¼Œä½¿ç”¨æ‰€æœ‰å›¾åƒ
            image_files = list(self.image_dir.glob("*.jpg")) + list(self.image_dir.glob("*.png"))
            self.image_names = [f.stem for f in image_files]
        
        # éªŒè¯æ•°æ®å­˜åœ¨æ€§
        valid_names = []
        for name in self.image_names:
            image_path = self._find_image_file(name)
            mask_path = self._find_mask_file(name)
            if image_path and mask_path:
                valid_names.append(name)
        
        self.image_names = valid_names
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(self.image_names)}")
    
    def _find_image_file(self, name):
        """æŸ¥æ‰¾å›¾åƒæ–‡ä»¶"""
        for ext in ['.jpg', '.jpeg', '.png']:
            path = self.image_dir / f"{name}{ext}"
            if path.exists():
                return path
        return None
    
    def _find_mask_file(self, name):
        """æŸ¥æ‰¾maskæ–‡ä»¶"""
        path = self.mask_dir / f"{name}.png"
        return path if path.exists() else None
    
    def __len__(self):
        return len(self.image_names)
    
    def __getitem__(self, idx):
        name = self.image_names[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = self._find_image_file(name)
        image = Image.open(image_path).convert('RGB')
        
        # åŠ è½½mask
        mask_path = self._find_mask_file(name)
        mask = Image.open(mask_path)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        mask_array = np.array(mask)
        
        # ç¡®ä¿maskå€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        mask_array = np.clip(mask_array, 0, 2)
        mask = Image.fromarray(mask_array)
        
        # åŒæ­¥å˜æ¢å›¾åƒå’Œmask
        if self.image_transform and self.mask_transform:
            # è·å–ç›¸åŒçš„éšæœºç§å­ç”¨äºåŒæ­¥å˜æ¢
            seed = np.random.randint(2147483647)
            
            # å˜æ¢å›¾åƒ
            np.random.seed(seed)
            torch.manual_seed(seed)
            image = self.image_transform(image)
            
            # ç”¨ç›¸åŒçš„éšæœºç§å­å˜æ¢mask
            np.random.seed(seed)
            torch.manual_seed(seed)
            mask_transformed = self.mask_transform(mask)
            mask = torch.from_numpy(np.array(mask_transformed)).long()
            
        elif self.image_transform:
            image = self.image_transform(image)
            mask = torch.from_numpy(np.array(mask)).long()
        else:
            # ä¸åº”ç”¨å˜æ¢ï¼Œç›´æ¥è½¬æ¢ä¸ºtensor
            image = transforms.ToTensor()(image)
            mask = torch.from_numpy(np.array(mask)).long()
        
        return image, mask


class SegmentationTransform:
    """åˆ†å‰²æ•°æ®å˜æ¢"""
    
    def __init__(self, config):
        self.config = config
        
    def get_train_transform(self):
        """è®­ç»ƒæ•°æ®å˜æ¢"""
        aug_config = self.config['data']['augmentation']
        
        transform_list = [
            transforms.Resize(aug_config['resize']),
        ]
        
        # éšæœºè£å‰ª
        if 'random_crop' in aug_config:
            transform_list.append(transforms.RandomCrop(aug_config['random_crop']))
        
        # éšæœºç¿»è½¬
        if aug_config.get('horizontal_flip', 0) > 0:
            transform_list.append(transforms.RandomHorizontalFlip(p=aug_config['horizontal_flip']))
        
        # é¢œè‰²å˜æ¢
        if 'color_jitter' in aug_config:
            jitter = aug_config['color_jitter']
            transform_list.append(transforms.ColorJitter(
                brightness=jitter.get('brightness', 0),
                contrast=jitter.get('contrast', 0),
                saturation=jitter.get('saturation', 0),
                hue=jitter.get('hue', 0)
            ))
        
        # è½¬æ¢ä¸ºå¼ é‡
        transform_list.extend([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        return transforms.Compose(transform_list)
    
    def get_val_transform(self):
        """éªŒè¯æ•°æ®å˜æ¢"""
        aug_config = self.config['data']['augmentation']
        
        # éªŒè¯æ—¶ä½¿ç”¨center cropç¡®ä¿å°ºå¯¸ä¸€è‡´
        target_size = aug_config.get('random_crop', aug_config['resize'])
        
        return transforms.Compose([
            transforms.Resize(aug_config['resize']),
            transforms.CenterCrop(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def get_mask_transform(self, is_training=True):
        """maskå˜æ¢"""
        aug_config = self.config['data']['augmentation']
        
        transform_list = [
            transforms.Resize(aug_config['resize'], interpolation=Image.NEAREST),
        ]
        
        if 'random_crop' in aug_config:
            if is_training:
                transform_list.append(transforms.RandomCrop(aug_config['random_crop']))
            else:
                # éªŒè¯æ—¶ä½¿ç”¨center crop
                transform_list.append(transforms.CenterCrop(aug_config['random_crop']))
        
        return transforms.Compose(transform_list)


class SegmentationMetrics:
    """åˆ†å‰²è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self, num_classes):
        self.num_classes = num_classes
        self.reset()
    
    def reset(self):
        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))
    
    def update(self, pred, target):
        """æ›´æ–°æ··æ·†çŸ©é˜µ"""
        pred = pred.cpu().numpy().flatten()
        target = target.cpu().numpy().flatten()
        
        # åªè€ƒè™‘æœ‰æ•ˆåƒç´ 
        valid_mask = (target >= 0) & (target < self.num_classes)
        pred = pred[valid_mask]
        target = target[valid_mask]
        
        # æ›´æ–°æ··æ·†çŸ©é˜µ
        for i in range(len(pred)):
            self.confusion_matrix[target[i], pred[i]] += 1
    
    def get_pixel_accuracy(self):
        """åƒç´ å‡†ç¡®ç‡"""
        acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()
        return acc
    
    def get_class_iou(self):
        """å„ç±»åˆ«IoU"""
        iou_per_class = []
        for i in range(self.num_classes):
            tp = self.confusion_matrix[i, i]
            fp = self.confusion_matrix[:, i].sum() - tp
            fn = self.confusion_matrix[i, :].sum() - tp
            
            if tp + fp + fn == 0:
                iou = 0.0
            else:
                iou = tp / (tp + fp + fn)
            iou_per_class.append(iou)
        
        return np.array(iou_per_class)
    
    def get_mean_iou(self):
        """å¹³å‡IoU"""
        return self.get_class_iou().mean()
    
    def get_dice_score(self):
        """Diceåˆ†æ•°"""
        dice_per_class = []
        for i in range(self.num_classes):
            tp = self.confusion_matrix[i, i]
            fp = self.confusion_matrix[:, i].sum() - tp
            fn = self.confusion_matrix[i, :].sum() - tp
            
            if 2 * tp + fp + fn == 0:
                dice = 0.0
            else:
                dice = 2 * tp / (2 * tp + fp + fn)
            dice_per_class.append(dice)
        
        return np.array(dice_per_class)


class SegmentationTrainer:
    """åˆ†å‰²è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = self._get_device()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_model()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader = self._create_data_loaders()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = self._create_optimizer()
        self.criterion = self._create_criterion()
        self.scheduler = self._create_scheduler()
        
        # åˆå§‹åŒ–TensorBoard
        if config['logging']['tensorboard']:
            self.writer = SummaryWriter(config['save']['log_dir'])
        else:
            self.writer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_miou = 0.0
        self.best_loss = float('inf')
        
    def _get_device(self):
        """è·å–è®¾å¤‡"""
        device_config = self.config['device']
        
        if device_config == 'mps' and torch.backends.mps.is_available():
            return torch.device('mps')
        elif device_config.startswith('cuda') and torch.cuda.is_available():
            return torch.device(device_config)
        else:
            return torch.device('cpu')
    
    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        dirs = [
            self.config['save']['checkpoint_dir'],
            self.config['save']['log_dir'],
            self.config['visualization']['prediction_dir']
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config['logging']
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=getattr(logging, log_config['level']),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[]
        )
        
        # æ§åˆ¶å°è¾“å‡º
        if log_config['console']:
            logging.getLogger().addHandler(logging.StreamHandler())
        
        # æ–‡ä»¶è¾“å‡º
        if log_config['file']:
            log_file = Path(self.config['save']['log_dir']) / 'training.log'
            logging.getLogger().addHandler(logging.FileHandler(log_file))
    
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        model_config = self.config['model']
        num_classes = model_config['num_classes']
        
        # åˆ›å»ºDeepLabV3+æ¨¡å‹
        if model_config['architecture'] == 'deeplabv3_resnet50':
            if model_config['pretrained']:
                # å…ˆåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆ21ç±»ï¼‰
                model = segmentation.deeplabv3_resnet50(pretrained=True)
                # æ›¿æ¢åˆ†ç±»å™¨ä¸ºæˆ‘ä»¬çš„ç±»åˆ«æ•°
                model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
                model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
            else:
                # ç›´æ¥åˆ›å»ºæŒ‡å®šç±»åˆ«æ•°çš„æ¨¡å‹
                model = segmentation.deeplabv3_resnet50(pretrained=False, num_classes=num_classes)
                
        elif model_config['architecture'] == 'deeplabv3_resnet101':
            if model_config['pretrained']:
                model = segmentation.deeplabv3_resnet101(pretrained=True)
                model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
                model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
            else:
                model = segmentation.deeplabv3_resnet101(pretrained=False, num_classes=num_classes)
                
        elif model_config['architecture'] == 'deeplabv3_mobilenet_v3_large':
            if model_config['pretrained']:
                model = segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
                model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
                model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
            else:
                model = segmentation.deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=num_classes)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ¶æ„: {model_config['architecture']}")
        
        model = model.to(self.device)
        logging.info(f"åˆ›å»ºæ¨¡å‹: {model_config['architecture']}, è®¾å¤‡: {self.device}")
        logging.info(f"è¾“å‡ºç±»åˆ«æ•°: {num_classes}, é¢„è®­ç»ƒ: {model_config['pretrained']}")
        
        return model
    
    def _create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        data_config = self.config['data']
        
        # åˆ›å»ºæ•°æ®å˜æ¢
        transform_manager = SegmentationTransform(self.config)
        train_transform = transform_manager.get_train_transform()
        val_transform = transform_manager.get_val_transform()
        train_mask_transform = transform_manager.get_mask_transform(is_training=True)
        val_mask_transform = transform_manager.get_mask_transform(is_training=False)
        
        # æ•°æ®åˆ†å‰²æ–‡ä»¶è·¯å¾„
        split_dir = Path(data_config['root_dir']) / data_config['split_dir']
        train_split = split_dir / 'train.txt' if split_dir.exists() else None
        val_split = split_dir / 'val.txt' if split_dir.exists() else None
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = SegmentationDataset(
            root_dir=data_config['root_dir'],
            image_dir=data_config['image_dir'],
            mask_dir=data_config['mask_dir'],
            split_file=train_split,
            transform=train_transform,
            mask_transform=train_mask_transform
        )
        
        val_dataset = SegmentationDataset(
            root_dir=data_config['root_dir'],
            image_dir=data_config['image_dir'],
            mask_dir=data_config['mask_dir'],
            split_file=val_split,
            transform=val_transform,
            mask_transform=val_mask_transform
        )
        
        # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†ä¸­åˆ†å‰²
        if len(val_dataset) == 0:
            logging.info("æ²¡æœ‰æ‰¾åˆ°éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†ä¸­åˆ†å‰²20%ä½œä¸ºéªŒè¯é›†")
            train_size = int(0.8 * len(train_dataset))
            val_size = len(train_dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(
                train_dataset, [train_size, val_size]
            )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        # MPSè®¾å¤‡ä¸æ”¯æŒpin_memory
        pin_memory = data_config['pin_memory'] and self.device.type != 'mps'
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=data_config['batch_size'],
            shuffle=data_config['shuffle'],
            num_workers=data_config['num_workers'],
            pin_memory=pin_memory
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=data_config['batch_size'],
            shuffle=False,
            num_workers=data_config['num_workers'],
            pin_memory=pin_memory
        )
        
        logging.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_loader, val_loader
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        train_config = self.config['training']
        
        # ç¡®ä¿æ•°å€¼å‚æ•°æ˜¯æ­£ç¡®çš„ç±»å‹
        learning_rate = float(train_config['learning_rate'])
        weight_decay = float(train_config['weight_decay'])
        momentum = float(train_config.get('momentum', 0.9))
        
        if train_config['optimizer'] == 'Adam':
            optimizer = optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif train_config['optimizer'] == 'AdamW':
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif train_config['optimizer'] == 'SGD':
            optimizer = optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=momentum,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {train_config['optimizer']}")
        
        logging.info(f"åˆ›å»ºä¼˜åŒ–å™¨: {train_config['optimizer']}, lr={learning_rate}, weight_decay={weight_decay}")
        return optimizer
    
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_config = self.config['training']['loss']
        
        if loss_config['type'] == 'CrossEntropyLoss':
            class_weights = loss_config.get('class_weights', None)
            if class_weights:
                class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)
            
            criterion = nn.CrossEntropyLoss(
                weight=class_weights,
                ignore_index=loss_config.get('ignore_index', -100)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_config['type']}")
        
        return criterion
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config['training']['scheduler']
        
        if scheduler_config['type'] == 'CosineAnnealingLR':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=int(scheduler_config['T_max']),
                eta_min=float(scheduler_config['eta_min'])
            )
        elif scheduler_config['type'] == 'StepLR':
            scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=int(scheduler_config.get('step_size', 30)),
                gamma=float(scheduler_config.get('gamma', 0.1))
            )
        elif scheduler_config['type'] == 'ReduceLROnPlateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=float(scheduler_config.get('factor', 0.5)),
                patience=int(scheduler_config.get('patience', 10))
            )
        else:
            return None
        
        logging.info(f"åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_config['type']}")
        return scheduler
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        metrics = SegmentationMetrics(self.config['model']['num_classes'])
        
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch + 1}')
        
        for batch_idx, (images, targets) in enumerate(progress_bar):
            images = images.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)['out']
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # æ›´æ–°æŒ‡æ ‡
            total_loss += loss.item()
            pred = torch.argmax(outputs, dim=1)
            metrics.update(pred, targets)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg Loss': f'{total_loss / (batch_idx + 1):.4f}'
            })
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)
        pixel_acc = metrics.get_pixel_accuracy()
        mean_iou = metrics.get_mean_iou()
        
        return avg_loss, pixel_acc, mean_iou
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0.0
        metrics = SegmentationMetrics(self.config['model']['num_classes'])
        
        with torch.no_grad():
            for images, targets in tqdm(self.val_loader, desc='Validation'):
                images = images.to(self.device)
                targets = targets.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(images)['out']
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(outputs, targets)
                total_loss += loss.item()
                
                # æ›´æ–°æŒ‡æ ‡
                pred = torch.argmax(outputs, dim=1)
                metrics.update(pred, targets)
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        pixel_acc = metrics.get_pixel_accuracy()
        mean_iou = metrics.get_mean_iou()
        class_iou = metrics.get_class_iou()
        dice_scores = metrics.get_dice_score()
        
        return avg_loss, pixel_acc, mean_iou, class_iou, dice_scores
    
    def save_checkpoint(self, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_miou': self.best_miou,
            'config': self.config
        }
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
        checkpoint_path = Path(self.config['save']['checkpoint_dir']) / f'checkpoint_epoch_{self.current_epoch}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = Path(self.config['save']['best_model_path'])
            best_path.parent.mkdir(parents=True, exist_ok=True)
            torch.save(checkpoint, best_path)
            logging.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def visualize_predictions(self, num_samples=None):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        if not self.config['visualization']['save_predictions']:
            return
        
        num_samples = num_samples or self.config['visualization']['num_samples']
        output_dir = Path(self.config['visualization']['prediction_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        self.model.eval()
        class_colors = self.config['visualization']['class_colors']
        
        with torch.no_grad():
            for i, (images, targets) in enumerate(self.val_loader):
                if i >= num_samples:
                    break
                
                images = images.to(self.device)
                outputs = self.model(images)['out']
                predictions = torch.argmax(outputs, dim=1)
                
                # å¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬
                image = images[0].cpu()
                target = targets[0].cpu().numpy()
                pred = predictions[0].cpu().numpy()
                
                # åå½’ä¸€åŒ–å›¾åƒ
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                image = image.permute(1, 2, 0).numpy()
                image = std * image + mean
                image = np.clip(image, 0, 1)
                
                # åˆ›å»ºå¯è§†åŒ–
                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                
                # åŸå›¾
                axes[0].imshow(image)
                axes[0].set_title('Original Image')
                axes[0].axis('off')
                
                # çœŸå®æ ‡ç­¾
                target_colored = self._colorize_mask(target, class_colors)
                axes[1].imshow(target_colored)
                axes[1].set_title('Ground Truth')
                axes[1].axis('off')
                
                # é¢„æµ‹ç»“æœ
                pred_colored = self._colorize_mask(pred, class_colors)
                axes[2].imshow(pred_colored)
                axes[2].set_title('Prediction')
                axes[2].axis('off')
                
                plt.tight_layout()
                plt.savefig(output_dir / f'prediction_epoch_{self.current_epoch}_sample_{i}.png')
                plt.close()
    
    def _colorize_mask(self, mask, class_colors):
        """ç»™maskç€è‰²"""
        h, w = mask.shape
        colored_mask = np.zeros((h, w, 3), dtype=np.uint8)
        
        for class_id, color in class_colors.items():
            colored_mask[mask == class_id] = color
        
        return colored_mask
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logging.info("å¼€å§‹åˆ†å‰²æ¨¡å‹è®­ç»ƒ")
        
        early_stopping_config = self.config['training']['early_stopping']
        patience = early_stopping_config['patience']
        min_delta = early_stopping_config['min_delta']
        no_improve_epochs = 0
        
        for epoch in range(self.config['training']['epochs']):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_acc, train_miou = self.train_epoch()
            
            # éªŒè¯
            if epoch % self.config['validation']['interval'] == 0:
                val_loss, val_acc, val_miou, class_iou, dice_scores = self.validate()
                
                # è®°å½•æ—¥å¿—
                logging.info(f'Epoch {epoch + 1}/{self.config["training"]["epochs"]}')
                logging.info(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, mIoU: {train_miou:.4f}')
                logging.info(f'Val - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, mIoU: {val_miou:.4f}')
                
                # TensorBoardè®°å½•
                if self.writer:
                    self.writer.add_scalar('Loss/Train', train_loss, epoch)
                    self.writer.add_scalar('Loss/Val', val_loss, epoch)
                    self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
                    self.writer.add_scalar('Accuracy/Val', val_acc, epoch)
                    self.writer.add_scalar('mIoU/Train', train_miou, epoch)
                    self.writer.add_scalar('mIoU/Val', val_miou, epoch)
                    
                    # å„ç±»åˆ«IoU
                    class_names = self.config['classes']['names']
                    for i, (name, iou) in enumerate(zip(class_names, class_iou)):
                        self.writer.add_scalar(f'IoU/{name}', iou, epoch)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = val_miou > self.best_miou + min_delta
                if is_best:
                    self.best_miou = val_miou
                    no_improve_epochs = 0
                    self.save_checkpoint(is_best=True)
                else:
                    no_improve_epochs += 1
                
                # æ—©åœæ£€æŸ¥
                if no_improve_epochs >= patience:
                    logging.info(f"æ—©åœè§¦å‘ï¼Œè¿ç»­{patience}ä¸ªepochæ— æ”¹å–„")
                    break
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config['save']['save_interval'] == 0:
                self.save_checkpoint()
            
            # å¯è§†åŒ–é¢„æµ‹ç»“æœ
            if epoch % 10 == 0:
                self.visualize_predictions()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
        
        logging.info("è®­ç»ƒå®Œæˆ!")
        
        # å¯¼å‡ºæ¨¡å‹
        self.export_models()
        
        if self.writer:
            self.writer.close()
    
    def export_models(self):
        """å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹"""
        export_config = self.config.get('export', {})
        if not export_config:
            return
        
        output_dir = Path(export_config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = self.config['save']['best_model_path']
        if os.path.exists(best_model_path):
            checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            logging.info(f"åŠ è½½æœ€ä½³æ¨¡å‹ç”¨äºå¯¼å‡º: {best_model_path}")
        
        self.model.eval()
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, 3, 224, 224).to(self.device)
        
        formats = export_config.get('formats', [])
        
        # å¯¼å‡ºONNX
        if 'onnx' in formats:
            onnx_path = output_dir / 'segmentation_model.onnx'
            onnx_config = export_config.get('onnx', {})
            
            try:
                torch.onnx.export(
                    self.model,
                    example_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=onnx_config.get('opset_version', 11),
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes=onnx_config.get('dynamic_axes', {
                        'input': {0: 'batch_size', 2: 'height', 3: 'width'},
                        'output': {0: 'batch_size', 2: 'height', 3: 'width'}
                    })
                )
                logging.info(f"ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {onnx_path}")
            except Exception as e:
                logging.error(f"ONNXå¯¼å‡ºå¤±è´¥: {e}")
        
        # å¯¼å‡ºTorchScript
        if 'torchscript' in formats:
            script_path = output_dir / 'segmentation_model.pt'
            try:
                traced_model = torch.jit.trace(self.model, example_input)
                traced_model.save(str(script_path))
                logging.info(f"TorchScriptæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {script_path}")
            except Exception as e:
                logging.error(f"TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
        
        # å¯¼å‡ºçº¯PyTorchæ¨¡å‹ï¼ˆåªä¿å­˜æ¨¡å‹ç»“æ„å’Œæƒé‡ï¼‰
        pytorch_path = output_dir / 'segmentation_model_pytorch.pth'
        try:
            # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
            model_info = {
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'architecture': self.config['model']['architecture'],
                    'num_classes': self.config['model']['num_classes'],
                    'pretrained': self.config['model']['pretrained']
                },
                'input_size': [224, 224],
                'class_names': self.config['classes']['names']
            }
            torch.save(model_info, pytorch_path)
            logging.info(f"PyTorchæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {pytorch_path}")
        except Exception as e:
            logging.error(f"PyTorchæ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="DeepLabV3+ åˆ†å‰²è®­ç»ƒ")
    parser.add_argument(
        "--config",
        type=str,
        default="config/segmentation_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--export-only",
        action="store_true",
        help="åªå¯¼å‡ºæ¨¡å‹ï¼Œä¸è¿›è¡Œè®­ç»ƒ"
    )
    parser.add_argument(
        "--export-formats",
        nargs='+',
        default=['onnx', 'torchscript'],
        help="å¯¼å‡ºæ ¼å¼åˆ—è¡¨"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SegmentationTrainer(config)
    
    if args.export_only:
        # åªå¯¼å‡ºæ¨¡å‹
        print("ğŸš€ å¼€å§‹å¯¼å‡ºæ¨¡å‹...")
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®ä¸­çš„å¯¼å‡ºæ ¼å¼
        if 'export' not in config:
            config['export'] = {}
        config['export']['formats'] = args.export_formats
        trainer.config = config
        
        trainer.export_models()
        print("âœ… æ¨¡å‹å¯¼å‡ºå®Œæˆ!")
    else:
        # æ­£å¸¸è®­ç»ƒæµç¨‹
        trainer.train()


if __name__ == "__main__":
    main()